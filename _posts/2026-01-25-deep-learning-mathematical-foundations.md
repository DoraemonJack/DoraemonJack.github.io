---
layout: post
title: "æ·±åº¦å­¦ä¹ â€”â€”æ•°å­¦åŸºç¡€ã€åå‘ä¼ æ’­ã€å¾®ç§¯åˆ†ä¸ä¼˜åŒ–ç†è®º"
subtitle: "ä»é“¾å¼æ³•åˆ™åˆ°æ¢¯åº¦æµåŠ¨ï¼Œæ·±å…¥ç†è§£ç¥ç»ç½‘ç»œçš„æ•°å­¦æœ¬è´¨ä¸å®é™…åº”ç”¨"
date: 2026-01-25
author: "DoraemonJack"
header-img: "img/post-bg-ml.jpg"
catalog:      true
mathjax:      true
mermaid:      true
tags:
    - æ·±åº¦å­¦ä¹ 
    - Deep Learning
    - æ•°å­¦ç†è®º
    - Mathematical Theory
    - åå‘ä¼ æ’­
    - Backpropagation
    - é“¾å¼æ³•åˆ™
    - Chain Rule
    - æ¢¯åº¦ä¸‹é™
    - Gradient Descent
    - å¾®ç§¯åˆ†
    - Calculus
    - ä¼˜åŒ–ç†è®º
    - Optimization Theory
    - ç¥ç»ç½‘ç»œ
    - Neural Networks
---

> æ·±åº¦å­¦ä¹ çš„ä¼˜é›…ä¹‹å¤„åœ¨äºå…¶ä¼˜ç¾çš„æ•°å­¦ç†è®ºã€‚åå‘ä¼ æ’­ç®—æ³•ä¸è¿‡æ˜¯é“¾å¼æ³•åˆ™çš„ä¸€ä¸ªå·§å¦™åº”ç”¨ï¼Œè€Œæ¢¯åº¦ä¸‹é™åˆ™æ˜¯å‡¸ä¼˜åŒ–ç†è®ºçš„ç›´æ¥æ¨è®ºã€‚æœ¬æ–‡å°†ä»ç¬¬ä¸€æ€§åŸç†å‡ºå‘ï¼Œé€å±‚æ·±å…¥è¿™äº›æ•°å­¦åŸºç¡€ï¼Œæœ€åé€šè¿‡å®é™…é¡¹ç›®æ¡ˆä¾‹å±•ç¤ºå¦‚ä½•å°†ç†è®ºåº”ç”¨äºå®è·µã€‚

## ç›®å½•

1. [æ·±åº¦å­¦ä¹ ä¸­çš„æ•°å­¦ä½“ç³»](#æ·±åº¦å­¦ä¹ ä¸­çš„æ•°å­¦ä½“ç³»)
2. [åŸºç¡€ 0ï¼šæ ¸å¿ƒæ•°å­¦åŸºç¡€é€Ÿé€š](#åŸºç¡€-0æ ¸å¿ƒæ•°å­¦åŸºç¡€é€Ÿé€š)
   - [çº¿æ€§ä»£æ•°æ ¸å¿ƒ](#çº¿æ€§ä»£æ•°æ ¸å¿ƒ)
   - [å¾®ç§¯åˆ†æ ¸å¿ƒ](#å¾®ç§¯åˆ†æ ¸å¿ƒ)
   - [æ¦‚ç‡ç»Ÿè®¡æ ¸å¿ƒ](#æ¦‚ç‡ç»Ÿè®¡æ ¸å¿ƒ)
3. [åŸºç¡€ 1ï¼šæ·±åº¦å­¦ä¹ ç®—æ³•ä¸æ¶æ„æ ¸å¿ƒ](#åŸºç¡€-1æ·±åº¦å­¦ä¹ ç®—æ³•ä¸æ¶æ„æ ¸å¿ƒ)
   - [æ„ŸçŸ¥æœºä¸å¤šå±‚æ„ŸçŸ¥æœº](#æ„ŸçŸ¥æœºä¸å¤šå±‚æ„ŸçŸ¥æœº)
   - [æ¿€æ´»å‡½æ•°çš„éçº¿æ€§æœ¬è´¨](#æ¿€æ´»å‡½æ•°çš„éçº¿æ€§æœ¬è´¨)
   - [æŸå¤±å‡½æ•°çš„é€‰æ‹©ä¸è®¾è®¡](#æŸå¤±å‡½æ•°çš„é€‰æ‹©ä¸è®¾è®¡)
   - [ä¼˜åŒ–ç®—æ³•åŸºç¡€](#ä¼˜åŒ–ç®—æ³•åŸºç¡€)
   - [åå‘ä¼ æ’­ç®—æ³•è¯¦è§£](#åå‘ä¼ æ’­ç®—æ³•è¯¦è§£)
4. [ç¬¬ä¸€éƒ¨åˆ†ï¼šå¾®ç§¯åˆ†åŸºç¡€](#ç¬¬ä¸€éƒ¨åˆ†å¾®ç§¯åˆ†åŸºç¡€)
5. [ç¬¬äºŒéƒ¨åˆ†ï¼šé“¾å¼æ³•åˆ™ä¸åå‘ä¼ æ’­](#ç¬¬äºŒéƒ¨åˆ†é“¾å¼æ³•åˆ™ä¸åå‘ä¼ æ’­)
6. [ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ¢¯åº¦ä¸‹é™ä¸ä¼˜åŒ–](#ç¬¬ä¸‰éƒ¨åˆ†æ¢¯åº¦ä¸‹é™ä¸ä¼˜åŒ–)
7. [ç¬¬å››éƒ¨åˆ†ï¼šæŸå¤±å‡½æ•°ä¸ä¿¡æ¯è®º](#ç¬¬å››éƒ¨åˆ†æŸå¤±å‡½æ•°ä¸ä¿¡æ¯è®º)
8. [ç¬¬äº”éƒ¨åˆ†ï¼šå®é™…æ¡ˆä¾‹â€”â€”ä»ç†è®ºåˆ°ä»£ç ](#ç¬¬äº”éƒ¨åˆ†å®é™…æ¡ˆä¾‹ä»ç†è®ºåˆ°ä»£ç )
9. [å¸¸è§é—®é¢˜ä¸æ·±åº¦æ´å¯Ÿ](#å¸¸è§é—®é¢˜ä¸æ·±åº¦æ´å¯Ÿ)

---

## æ·±åº¦å­¦ä¹ ä¸­çš„æ•°å­¦ä½“ç³»

### æ•°å­¦ç†è®ºçš„å±‚çº§ç»“æ„

æ·±åº¦å­¦ä¹ çš„æ•°å­¦åŸºç¡€å¯ä»¥åˆ†ä¸ºå››ä¸ªå±‚çº§ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åº”ç”¨å±‚ï¼šç¥ç»ç½‘ç»œæ¶æ„è®¾è®¡             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ç®—æ³•å±‚ï¼šä¼˜åŒ–æ–¹æ³•ï¼ˆSGDã€Adamç­‰ï¼‰      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ç†è®ºå±‚ï¼šå‡¸ä¼˜åŒ–ã€æ³›å‡½åˆ†æ              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  åŸºç¡€å±‚ï¼šå¾®ç§¯åˆ†ã€çº¿æ€§ä»£æ•°ã€æ¦‚ç‡è®º      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

è¿™ç¯‡æ–‡ç« å°†é‡ç‚¹å…³æ³¨**åŸºç¡€å±‚å’Œç†è®ºå±‚**ï¼Œè¿™æ˜¯æ‰€æœ‰é«˜é˜¶æŠ€å·§çš„æ•°å­¦åŸºç¡€ã€‚

---

## åŸºç¡€ 0ï¼šæ ¸å¿ƒæ•°å­¦åŸºç¡€é€Ÿé€š

> æœ¬èŠ‚ä¸“ä¸ºåªæƒ³å¿«é€ŸæŒæ¡æ ¸å¿ƒæ¦‚å¿µçš„è¯»è€…è®¾è®¡ã€‚å¦‚æœä½ å·²ç»ç†Ÿæ‚‰è¿™äº›åŸºç¡€ï¼Œå¯ä»¥ç›´æ¥è·³åˆ°"ç¬¬ä¸€éƒ¨åˆ†"ã€‚

### çº¿æ€§ä»£æ•°æ ¸å¿ƒ

#### 1. çŸ©é˜µä¹˜æ³•â€”â€”ç¥ç»ç½‘ç»œè®¡ç®—çš„åŸºçŸ³

**å®šä¹‰**ï¼šçŸ©é˜µ $A \in \mathbb{R}^{m \times n}$ ä¸çŸ©é˜µ $B \in \mathbb{R}^{n \times p}$ çš„ä¹˜ç§¯ $C = AB \in \mathbb{R}^{m \times p}$ å®šä¹‰ä¸ºï¼š

$$C_{ij} = \sum_{k=1}^{n} A_{ik} B_{kj}$$

**ç¥ç»ç½‘ç»œä¸­çš„æ„ä¹‰**ï¼š
- è¾“å…¥å‘é‡ $\mathbf{x} \in \mathbb{R}^n$
- æƒé‡çŸ©é˜µ $\mathbf{W} \in \mathbb{R}^{m \times n}$
- è¾“å‡º $\mathbf{y} = \mathbf{W}\mathbf{x} + \mathbf{b}$ å°±æ˜¯çŸ©é˜µä¹˜æ³•åŠ åç½®

**è®¡ç®—å¤æ‚æ€§**ï¼š$O(mnp)$ â€” è¿™æ˜¯ä¸ºä»€ä¹ˆå¤§æ¨¡å‹è¿™ä¹ˆæ…¢çš„æ ¹æœ¬åŸå› ï¼

**å®é™…ä¾‹å­**ï¼š
```
ä¸€ä¸ªç¥ç»å…ƒå±‚çš„è®¡ç®—ï¼š
è¾“å…¥ï¼šx âˆˆ â„^100  (100 ä¸ªç‰¹å¾)
æƒé‡ï¼šW âˆˆ â„^64Ã—100  (64 ä¸ªç¥ç»å…ƒ)
åç½®ï¼šb âˆˆ â„^64

è®¡ç®—ï¼šy = WÂ·x + b
ç»“æœï¼šy âˆˆ â„^64  (64 ä¸ªè¾“å‡º)

çŸ©é˜µä¹˜æ³•è¿ç®—æ•°ï¼š64 Ã— 100 = 6400 æ¬¡ä¹˜æ³•
```

#### 2. ç‰¹å¾å€¼ä¸ç‰¹å¾å‘é‡â€”â€”ç†è§£çŸ©é˜µçš„æœ¬è´¨

**å®šä¹‰**ï¼šå¯¹äºçŸ©é˜µ $A \in \mathbb{R}^{n \times n}$ï¼Œå¦‚æœå­˜åœ¨æ ‡é‡ $\lambda$ å’Œå‘é‡ $\mathbf{v} \neq \mathbf{0}$ æ»¡è¶³ï¼š

$$A\mathbf{v} = \lambda \mathbf{v}$$

åˆ™ $\lambda$ æ˜¯ $A$ çš„**ç‰¹å¾å€¼**ï¼Œ$\mathbf{v}$ æ˜¯å¯¹åº”çš„**ç‰¹å¾å‘é‡**ã€‚

**å‡ ä½•ç›´è§‚**ï¼šçŸ©é˜µä½œç”¨åœ¨ç‰¹å¾å‘é‡ä¸Šï¼Œåªæ˜¯æ”¹å˜å…¶å¤§å°ï¼ˆå€æ•°ä¸º $\lambda$ï¼‰ï¼Œä¸æ”¹å˜æ–¹å‘ã€‚

**åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨**ï¼š

1. **Hessian çŸ©é˜µçš„ç‰¹å¾å€¼**å†³å®šäº†äºŒé˜¶ä¼˜åŒ–æ–¹æ³•çš„æ”¶æ•›é€Ÿåº¦ï¼š
   - ç‰¹å¾å€¼éƒ½å¾ˆå° â†’ æ”¶æ•›å¿«
   - ç‰¹å¾å€¼å·®å¼‚å¤§ï¼ˆæ¡ä»¶æ•° $\kappa = \frac{\lambda_{\max}}{\lambda_{\min}}$ å¤§ï¼‰â†’ æ”¶æ•›æ…¢

2. **ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰**ä¸­ï¼šç‰¹å¾å‘é‡æŒ‡å‘æ–¹å·®æœ€å¤§çš„æ–¹å‘

**è®¡ç®—ä¾‹å­**ï¼š
$$A = \begin{pmatrix} 4 & 1 \\ 1 & 3 \end{pmatrix}$$

ç‰¹å¾æ–¹ç¨‹ï¼š$\det(A - \lambda I) = 0$
$$(4-\lambda)(3-\lambda) - 1 = 0$$
$$\lambda^2 - 7\lambda + 11 = 0$$
$$\lambda_1 = \frac{7 + \sqrt{5}}{2} \approx 5.618, \quad \lambda_2 = \frac{7 - \sqrt{5}}{2} \approx 1.382$$

#### 3. çŸ©é˜µè½¬ç½®â€”â€”ä¿¡æ¯çš„åå‘æµåŠ¨

**å®šä¹‰**ï¼šçŸ©é˜µ $A$ çš„è½¬ç½® $A^T$ å®šä¹‰ä¸º $(A^T)_{ij} = A_{ji}$

**å…³é”®æ€§è´¨**ï¼š
$$(\mathbf{A}\mathbf{B})^T = \mathbf{B}^T\mathbf{A}^T$$

è¿™åœ¨åå‘ä¼ æ’­ä¸­è‡³å…³é‡è¦ï¼

**åœ¨åå‘ä¼ æ’­ä¸­çš„ä½“ç°**ï¼š

è®¾æ­£å‘ä¼ æ’­ä¸º $\mathbf{z} = \mathbf{W}\mathbf{x} + \mathbf{b}$ï¼ŒæŸå¤±ä¸º $L$ã€‚

åˆ™ï¼š
$$\frac{\partial L}{\partial \mathbf{x}} = \frac{\partial L}{\partial \mathbf{z}} \frac{\partial \mathbf{z}}{\partial \mathbf{x}} = \mathbf{W}^T \frac{\partial L}{\partial \mathbf{z}}$$

æ³¨æ„æ¢¯åº¦é€šè¿‡ $\mathbf{W}^T$ åå‘ä¼ æ’­ï¼

#### 4. çŸ©é˜µçš„ç§©â€”â€”æ•°æ®ç»´åº¦çš„çœŸç›¸

**å®šä¹‰**ï¼šçŸ©é˜µçš„ç§© $\text{rank}(A)$ æ˜¯å…¶è¡Œå‘é‡ï¼ˆæˆ–åˆ—å‘é‡ï¼‰çš„æœ€å¤§çº¿æ€§æ— å…³ä¸ªæ•°ã€‚

**å…³é”®æ€§è´¨**ï¼š
- å¯¹äº $A \in \mathbb{R}^{m \times n}$ï¼š$\text{rank}(A) \leq \min(m, n)$
- å¦‚æœ $\text{rank}(A) = \min(m, n)$ï¼Œåˆ™ç§°çŸ©é˜µæ»¡ç§©ï¼ˆfull rankï¼‰
- æ»¡ç§©çŸ©é˜µå¯é€†

**åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„æ„ä¹‰**ï¼š

1. **é¿å…ç§©äº**ï¼šå¦‚æœæƒé‡çŸ©é˜µç§©å¤ªä½ï¼Œä¿¡æ¯ä¼šä¸¢å¤±
   ```
   ç§©ä¸º 1 çš„æƒé‡çŸ©é˜µï¼š
   W = uÂ·v^T  (å¤–ç§¯)
   
   è¿™æ„å‘³ç€è¾“å‡ºåœ¨ä¸€ç»´å­ç©ºé—´ä¸­ï¼Œ
   æ— è®ºè¾“å…¥å¦‚ä½•ï¼Œéƒ½åœ¨è¿™ä¸ªæ–¹å‘ä¸Šã€‚
   è¿™éå¸¸é™åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚
   ```

2. **å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰**ï¼š$A = U\Sigma V^T$ï¼Œç§©ç­‰äº $\Sigma$ ä¸­éé›¶å…ƒç´ ä¸ªæ•°

3. **è¿‡å‚æ•°åŒ–ç†è®º**ï¼šæ·±å±‚ç½‘ç»œçš„æƒé‡çŸ©é˜µé€šå¸¸ç§©è¾ƒä½ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡ï¼

**å®ä¾‹**ï¼š
```
çŸ©é˜µ A = [1 2]  ç§© = 1
         [2 4]  (ç¬¬äºŒè¡Œæ˜¯ç¬¬ä¸€è¡Œçš„ 2 å€)

çŸ©é˜µ B = [1 0]  ç§© = 2
         [0 1]  (æ»¡ç§©ï¼Œå¯é€†)
```

#### å°ç»“ï¼šçŸ©é˜µè¿ç®—çš„å†…å­˜/è®¡ç®—å¤æ‚æ€§

| æ“ä½œ | å¤æ‚æ€§ | å¤‡æ³¨ |
|------|--------|------|
| çŸ©é˜µä¹˜æ³• $A_{mÃ—n} \times B_{nÃ—p}$ | $O(mnp)$ | ç“¶é¢ˆæ“ä½œï¼ŒGPU ä¼˜åŒ– |
| çŸ©é˜µè½¬ç½® | $O(mn)$ | å†…å­˜ä¹Ÿæ˜¯ $O(mn)$ |
| ç‰¹å¾å€¼åˆ†è§£ | $O(n^3)$ | è®¡ç®—ä»£ä»·å¤§ï¼Œå¾ˆå°‘ç”¨äºå¤§è§„æ¨¡ç½‘ç»œ |
| æ±‚é€† | $O(n^3)$ | æ•°å€¼ä¸ç¨³å®šï¼Œé€šå¸¸ç”¨æ±‚è§£è€Œéæ±‚é€† |

---

### å¾®ç§¯åˆ†æ ¸å¿ƒ

#### 1. å¯¼æ•°ä¸åå¯¼æ•°â€”â€”å˜åŒ–ç‡çš„åº¦é‡

**å•å˜é‡å¯¼æ•°**ï¼š
$$f'(x_0) = \lim_{h \to 0} \frac{f(x_0 + h) - f(x_0)}{h}$$

**å‡ ä½•æ„ä¹‰**ï¼šæ›²çº¿åœ¨è¯¥ç‚¹çš„åˆ‡çº¿æ–œç‡ã€‚

**å¤šå˜é‡åå¯¼æ•°**ï¼šå¯¹äº $f(x_1, x_2, \ldots, x_n)$ï¼š

$$\frac{\partial f}{\partial x_i} = \lim_{h \to 0} \frac{f(x_1, \ldots, x_i+h, \ldots, x_n) - f(x_1, \ldots, x_n)}{h}$$

**å…³é”®æ€§è´¨**ï¼šæ··åˆåå¯¼æ•°ç›¸ç­‰ï¼ˆè¿ç»­å‡½æ•°ï¼‰
$$\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}$$

#### 2. æ¢¯åº¦â€”â€”æœ€é™¡å³­çš„æ”€ç™»æ–¹å‘

**å®šä¹‰**ï¼šæ¢¯åº¦æ˜¯æ‰€æœ‰åå¯¼æ•°çš„åˆ—å‘é‡ï¼š

$$\nabla f = \begin{pmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{pmatrix}$$

**æœ€é‡è¦çš„æ€§è´¨**ï¼š
- **æ–¹å‘**ï¼šæŒ‡å‘å‡½æ•°å¢é•¿æœ€å¿«çš„æ–¹å‘
- **å¤§å°**ï¼š$\|\nabla f\|$ è¡¨ç¤ºå¢é•¿é€Ÿç‡
- **æ–¹å‘å¯¼æ•°**ï¼šåœ¨æ–¹å‘ $\mathbf{u}$ ä¸Šçš„å¯¼æ•° = $\nabla f \cdot \mathbf{u}$

**åœ¨ç¥ç»ç½‘ç»œä¸­**ï¼š
```
ç¥ç»ç½‘ç»œæŸå¤±å‡½æ•° L(Î¸â‚, Î¸â‚‚, ..., Î¸_M)
å…¶ä¸­ M å¯èƒ½æ˜¯æ•°ç™¾ä¸‡ä¸ªå‚æ•°

æ¢¯åº¦ âˆ‡L å°±æ˜¯è¿™æ•°ç™¾ä¸‡ä¸ªåå¯¼æ•°çš„å‘é‡

æ¢¯åº¦ä¸‹é™ï¼šÎ¸_{t+1} = Î¸_t - Î±Â·âˆ‡L(Î¸_t)
```

#### 3. é“¾å¼æ³•åˆ™â€”â€”åå‘ä¼ æ’­çš„æ•°å­¦æœ¬è´¨

**ä¸€ç»´æƒ…å†µ**ï¼šå¦‚æœ $y = f(g(x))$ï¼Œåˆ™ï¼š
$$\frac{dy}{dx} = \frac{dy}{dg} \cdot \frac{dg}{dx}$$

**å¤šç»´æƒ…å†µ**ï¼šå¯¹äº $y = f(\mathbf{g}(\mathbf{x}))$ï¼Œå…¶ä¸­ï¼š
- $\mathbf{x} \in \mathbb{R}^n$
- $\mathbf{g}: \mathbb{R}^n \to \mathbb{R}^m$  
- $f: \mathbb{R}^m \to \mathbb{R}$

æ¢¯åº¦å…³ç³»ï¼ˆçŸ©é˜µå½¢å¼ï¼‰ï¼š
$$\nabla_{\mathbf{x}} f = J_{\mathbf{g}}^T \nabla_{\mathbf{g}} f$$

å…¶ä¸­ $J_{\mathbf{g}}$ æ˜¯ $\mathbf{g}$ çš„ **Jacobian çŸ©é˜µ**ï¼š

$$J = \begin{pmatrix}
\frac{\partial g_1}{\partial x_1} & \cdots & \frac{\partial g_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial g_m}{\partial x_1} & \cdots & \frac{\partial g_m}{\partial x_n}
\end{pmatrix}$$

**ä¸ºä»€ä¹ˆé“¾å¼æ³•åˆ™æ˜¯åå‘ä¼ æ’­ï¼Ÿ**

ç¥ç»ç½‘ç»œæ˜¯å‡½æ•°å¤åˆï¼š$y = f_L(f_{L-1}(\cdots f_1(\mathbf{x})))$

åº”ç”¨é“¾å¼æ³•åˆ™ï¼š
$$\frac{\partial L}{\partial \mathbf{x}} = \frac{\partial L}{\partial f_L} \cdot \frac{\partial f_L}{\partial f_{L-1}} \cdot \ldots \cdot \frac{\partial f_1}{\partial \mathbf{x}}$$

è¿™æ­£æ˜¯åå‘ä¼ æ’­ï¼šä»è¾“å‡ºå±‚é€å±‚å‘è¾“å…¥å±‚ä¼ æ’­æ¢¯åº¦ï¼

#### 4. æ–¹å‘å¯¼æ•°ä¸æ¢¯åº¦çš„å…³ç³»

æ–¹å‘å¯¼æ•°ï¼ˆæ²¿å•ä½å‘é‡ $\mathbf{u}$ çš„æ–¹å‘ï¼‰ï¼š
$$D_{\mathbf{u}} f = \nabla f \cdot \mathbf{u} = \|\nabla f\| \cos\theta$$

å…¶ä¸­ $\theta$ æ˜¯æ¢¯åº¦ä¸æ–¹å‘çš„å¤¹è§’ã€‚

**ä¸‰ä¸ªå…³é”®è§’åº¦**ï¼š
- $\theta = 0Â°$ï¼šæ²¿æ¢¯åº¦æ–¹å‘ï¼Œ$D_{\mathbf{u}}f = \|\nabla f\|$ï¼ˆæœ€å¿«å¢é•¿ï¼‰
- $\theta = 90Â°$ï¼šå‚ç›´äºæ¢¯åº¦ï¼Œ$D_{\mathbf{u}}f = 0$ï¼ˆå‡½æ•°å€¼ä¸å˜ï¼‰
- $\theta = 180Â°$ï¼šé€†æ¢¯åº¦æ–¹å‘ï¼Œ$D_{\mathbf{u}}f = -\|\nabla f\|$ï¼ˆæœ€å¿«ä¸‹é™ï¼‰

è¿™æ­£æ˜¯æ¢¯åº¦ä¸‹é™çš„ç†è®ºåŸºç¡€ï¼

---

### æ¦‚ç‡ç»Ÿè®¡æ ¸å¿ƒ

#### 1. æ­£æ€åˆ†å¸ƒâ€”â€”æ·±åº¦å­¦ä¹ ä¸­çš„ä¸»è§’

**æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDFï¼‰**ï¼š
$$f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$

**è®°å·**ï¼š$x \sim \mathcal{N}(\mu, \sigma^2)$

**å‚æ•°æ„ä¹‰**ï¼š
- $\mu$ï¼šå‡å€¼ï¼ˆä¸­å¿ƒä½ç½®ï¼‰
- $\sigma^2$ï¼šæ–¹å·®ï¼ˆç¦»æ•£ç¨‹åº¦ï¼‰
- $\sigma$ï¼šæ ‡å‡†å·®

**æ ‡å‡†æ­£æ€åˆ†å¸ƒ**ï¼š$\mu = 0, \sigma = 1$ï¼Œè®°ä¸º $Z \sim \mathcal{N}(0,1)$

**å¤šå…ƒæ­£æ€åˆ†å¸ƒ**ï¼š$\mathbf{x} \in \mathbb{R}^n$ æ»¡è¶³ï¼š

$$f(\mathbf{x}) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T\Sigma^{-1}(\mathbf{x}-\boldsymbol{\mu})\right)$$

å…¶ä¸­ $\Sigma$ æ˜¯åæ–¹å·®çŸ©é˜µã€‚

**åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨**ï¼š

1. **æƒé‡åˆå§‹åŒ–**ï¼ˆHe åˆå§‹åŒ–ã€Xavier åˆå§‹åŒ–ï¼‰ï¼šä»æ­£æ€åˆ†å¸ƒé‡‡æ ·
   ```python
   # He åˆå§‹åŒ–ï¼ˆé’ˆå¯¹ ReLUï¼‰
   W ~ N(0, 2/n_in)
   
   # è¿™ä¸ªæ–¹å·®çš„é€‰æ‹©åŸºäºç»Ÿè®¡åˆ†æ
   # ç¡®ä¿æ¢¯åº¦åœ¨ç½‘ç»œä¼ æ’­æ—¶ä¸ä¼šçˆ†ç‚¸æˆ–æ¶ˆå¤±
   ```

2. **æ‰¹é‡å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰**ï¼šå°†æ¿€æ´»å€¼å½’ä¸€åŒ–ä¸ºæ¥è¿‘æ ‡å‡†æ­£æ€

3. **Dropout æ­£åˆ™åŒ–**ï¼šéšæœºä¸¢å¼ƒç¥ç»å…ƒï¼Œç­‰ä»·äºé›†æˆå¤šä¸ªæ¨¡å‹

#### 2. æœŸæœ›ä¸æ–¹å·®â€”â€”æ•°æ®ç‰¹å¾çš„é‡åŒ–

**æœŸæœ›ï¼ˆå‡å€¼ï¼‰**ï¼š
$$\mathbb{E}[X] = \int_{-\infty}^{\infty} x f(x) dx$$

**æ–¹å·®**ï¼š
$$\text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$$

**æ ‡å‡†å·®**ï¼š$\sigma = \sqrt{\text{Var}(X)}$

**åæ–¹å·®**ï¼ˆä¸¤ä¸ªå˜é‡çš„ç›¸å…³æ€§ï¼‰ï¼š
$$\text{Cov}(X, Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])]$$

**ç›¸å…³ç³»æ•°**ï¼ˆæ— é‡çº²çš„åæ–¹å·®ï¼‰ï¼š
$$\rho = \frac{\text{Cov}(X,Y)}{\sigma_X \sigma_Y} \in [-1, 1]$$

**åœ¨ç¥ç»ç½‘ç»œä¸­çš„æ„ä¹‰**ï¼š

```
å‡è®¾æŸå±‚çš„æ¿€æ´»å€¼ z ~ N(Î¼, ÏƒÂ²)

- å¦‚æœ Ïƒ å¾ˆå¤§ï¼šæ¿€æ´»å€¼æç«¯åˆ†æ•£
  â†’ Sigmoid é¥±å’Œï¼ˆæ¢¯åº¦æ¥è¿‘ 0ï¼‰
  â†’ å­¦ä¹ ç¼“æ…¢

- å¦‚æœ Ïƒ å¾ˆå°ï¼šæ¿€æ´»å€¼èšé›†
  â†’ å¾ˆå¤šä¿¡æ¯ä¸¢å¤±
  â†’ ç½‘ç»œè¡¨è¾¾èƒ½åŠ›å—é™

- æœ€ä¼˜æƒ…å†µï¼šÏƒ â‰ˆ 1ï¼ˆæ ‡å‡†åŒ–ï¼‰
  â†’ æ¢¯åº¦æµåŠ¨æœ€ä¼˜
  â†’ å­¦ä¹ æœ€å¿«
```

#### 3. æ¦‚ç‡åˆ†å¸ƒçš„KLæ•£åº¦â€”â€”è¡¡é‡åˆ†å¸ƒå·®å¼‚

**KL æ•£åº¦ï¼ˆç›¸å¯¹ç†µï¼‰**ï¼š

$$D_{KL}(P \| Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}$$

å¯¹è¿ç»­åˆ†å¸ƒï¼š
$$D_{KL}(P \| Q) = \int P(x) \log \frac{P(x)}{Q(x)} dx$$

**å…³é”®æ€§è´¨**ï¼š
- $D_{KL}(P \| Q) \geq 0$ï¼Œç­‰å·æˆç«‹å½“ä¸”ä»…å½“ $P = Q$
- **ä¸å¯¹ç§°æ€§**ï¼š$D_{KL}(P \| Q) \neq D_{KL}(Q \| P)$

**æ¨å¯¼äº¤å‰ç†µ**ï¼š

$$D_{KL}(P \| Q) = \sum_x P(x)[\log P(x) - \log Q(x)]$$
$$= -H(P) + H(P,Q)$$

å…¶ä¸­ï¼š
- $H(P) = -\sum_x P(x) \log P(x)$ æ˜¯ $P$ çš„ç†µï¼ˆå¸¸æ•°ï¼ŒçœŸå®åˆ†å¸ƒå›ºå®šï¼‰
- $H(P,Q) = -\sum_x P(x) \log Q(x)$ æ˜¯äº¤å‰ç†µï¼ˆå¯ä¼˜åŒ–ï¼‰

å› æ­¤ï¼šæœ€å°åŒ– KL æ•£åº¦ = æœ€å°åŒ–äº¤å‰ç†µ

#### 4. è´å¶æ–¯å®šç†â€”â€”ä»å…ˆéªŒåˆ°åéªŒ

**è´å¶æ–¯å®šç†**ï¼š
$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$

**æœ¯è¯­**ï¼š
- $P(A)$ï¼šå…ˆéªŒï¼ˆå…ˆå‰çš„ä¿¡å¿µï¼‰
- $P(B|A)$ï¼šä¼¼ç„¶ï¼ˆè§‚æµ‹åˆ°æ•°æ®çš„å¯èƒ½æ€§ï¼‰
- $P(A|B)$ï¼šåéªŒï¼ˆæ›´æ–°åçš„ä¿¡å¿µï¼‰
- $P(B)$ï¼šè¯æ®ï¼ˆå¸¸æ•°ï¼Œå½’ä¸€åŒ–å› å­ï¼‰

**åœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨**ï¼š

è®¾ $\theta$ æ˜¯æ¨¡å‹å‚æ•°ï¼Œ$D$ æ˜¯è§‚æµ‹æ•°æ®ï¼š

$$P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}$$

**æœ€å¤§åéªŒä¼°è®¡ï¼ˆMAPï¼‰**ï¼š
$$\theta^* = \arg\max_\theta P(\theta|D) = \arg\max_\theta P(D|\theta)P(\theta)$$

**è´å¶æ–¯è§£é‡Š L2 æ­£åˆ™åŒ–**ï¼š

å‡è®¾å‚æ•°å…ˆéªŒï¼š$P(\theta) = \mathcal{N}(0, \sigma^2)$

```
-log P(Î¸|D) = -log P(D|Î¸) - log P(Î¸) + const
            âˆ L(Î¸) + Î»||Î¸||â‚‚Â²
```

å› æ­¤ï¼ŒL2 æ­£åˆ™åŒ–ç­‰ä»·äºé«˜æ–¯å…ˆéªŒï¼å‚æ•° $\lambda$ åæ˜ äº†æˆ‘ä»¬å¯¹å‚æ•°å¤§å°çš„å…ˆéªŒä¿¡å¿µã€‚

#### 5. å¤§æ•°å®šå¾‹ä¸ä¸­å¿ƒæé™å®šç†

**å¤§æ•°å®šå¾‹**ï¼šè®¾ $X_1, X_2, \ldots$ ç‹¬ç«‹åŒåˆ†å¸ƒï¼Œ$\mathbb{E}[X_i] = \mu$ï¼Œåˆ™ï¼š

$$\frac{1}{n}\sum_{i=1}^n X_i \xrightarrow{p} \mu \quad \text{å½“ } n \to \infty$$

**åœ¨ SGD ä¸­çš„åº”ç”¨**ï¼šå°æ‰¹é‡çš„å¹³å‡æ¢¯åº¦è¶‹å‘äºå…¨æ‰¹é‡æ¢¯åº¦

**ä¸­å¿ƒæé™å®šç†ï¼ˆCLTï¼‰**ï¼šæ ·æœ¬å‡å€¼çš„åˆ†å¸ƒè¶‹å‘äºæ­£æ€åˆ†å¸ƒ

$$\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i \xrightarrow{d} \mathcal{N}\left(\mu, \frac{\sigma^2}{n}\right)$$

**åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„å«ä¹‰**ï¼š
- å³ä½¿åŸå§‹æ•°æ®åˆ†å¸ƒä¸æ­£æ€ï¼Œæ¿€æ´»å€¼çš„åˆ†å¸ƒä¹Ÿä¼šè¶‹å‘æ­£æ€ï¼ˆåœ¨æ·±å±‚ç½‘ç»œä¸­ï¼‰
- è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆæ‰¹é‡å½’ä¸€åŒ–ä¼šå‡è®¾æ•°æ®è¿‘ä¼¼æ­£æ€åˆ†å¸ƒ
- è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆæƒé‡åˆå§‹åŒ–æ—¶ä½¿ç”¨æ­£æ€åˆ†å¸ƒæ˜¯åˆç†çš„

---

### æ ¸å¿ƒæ¦‚å¿µé€ŸæŸ¥è¡¨

| æ¦‚å¿µ | å®šä¹‰ | åœ¨ç¥ç»ç½‘ç»œä¸­çš„ä½œç”¨ |
|------|------|------|
| **çŸ©é˜µä¹˜æ³•** | $C_{ij} = \sum_k A_{ik}B_{kj}$ | å‰å‘ä¼ æ’­çš„åŸºæœ¬è®¡ç®— |
| **ç‰¹å¾å€¼** | $A\mathbf{v} = \lambda\mathbf{v}$ | ä¼˜åŒ–é€Ÿåº¦ï¼ˆæ¡ä»¶æ•°ï¼‰ |
| **ç§©** | çº¿æ€§æ— å…³è¡Œ/åˆ—çš„ä¸ªæ•° | æ•°æ®ç»´åº¦å’Œè¡¨è¾¾èƒ½åŠ› |
| **è½¬ç½®** | $(A^T)_{ij} = A_{ji}$ | åå‘ä¼ æ’­æ—¶æ¢¯åº¦æµå‘ |
| **æ¢¯åº¦** | $\nabla f = [\partial_1 f, \ldots, \partial_n f]^T$ | ä¼˜åŒ–çš„æ–¹å‘å’Œé€Ÿåº¦ |
| **é“¾å¼æ³•åˆ™** | $\frac{dy}{dx} = \frac{dy}{dz}\frac{dz}{dx}$ | åå‘ä¼ æ’­çš„æ•°å­¦æœ¬è´¨ |
| **æ­£æ€åˆ†å¸ƒ** | $f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{(x-\mu)^2}{2\sigma^2})$ | æƒé‡åˆå§‹åŒ–ã€æ•°æ®å‡è®¾ |
| **æ–¹å·®** | $\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$ | æ•°æ®æ ‡å‡†åŒ– |
| **KL æ•£åº¦** | $D_{KL}(P\|Q) = \sum P(x)\log\frac{P(x)}{Q(x)}$ | äº¤å‰ç†µæŸå¤±å‡½æ•° |
| **è´å¶æ–¯å®šç†** | $P(A\|B) = \frac{P(B\|A)P(A)}{P(B)}$ | L2 æ­£åˆ™åŒ–çš„è§£é‡Š |

---

## åŸºç¡€ 1ï¼šæ·±åº¦å­¦ä¹ ç®—æ³•ä¸æ¶æ„æ ¸å¿ƒ

æœ¬èŠ‚ä»ç®—æ³•å’Œæ¶æ„çš„è§’åº¦ï¼Œè®²è§£æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæ„ä»¶ã€‚è¿™æ˜¯ä»"çº¯æ•°å­¦"åˆ°"å¯å·¥ä½œçš„æ¨¡å‹"çš„æ¡¥æ¢ã€‚

### æ„ŸçŸ¥æœºä¸å¤šå±‚æ„ŸçŸ¥æœº

#### 1. å•å±‚æ„ŸçŸ¥æœºï¼ˆPerceptronï¼‰â€”â€”ç¬¬ä¸€ä¸ªç¥ç»ç½‘ç»œ

**å†å²èƒŒæ™¯**ï¼šæ„ŸçŸ¥æœºæ˜¯ Rosenblatt åœ¨ 1958 å¹´æå‡ºçš„ï¼Œæ˜¯ç°ä»£ç¥ç»ç½‘ç»œçš„ç¥–å…ˆã€‚

**æ¨¡å‹å®šä¹‰**ï¼š

ç»™å®šè¾“å…¥ $\mathbf{x} \in \mathbb{R}^n$ å’Œæƒé‡ $\mathbf{w} \in \mathbb{R}^n$ã€åç½® $b \in \mathbb{R}$ï¼š

$$z = \mathbf{w} \cdot \mathbf{x} + b = \sum_{i=1}^n w_i x_i + b$$

$$\hat{y} = \text{sign}(z) = \begin{cases} 1 & \text{if } z > 0 \\ -1 & \text{otherwise} \end{cases}$$

å…¶ä¸­ $\text{sign}$ æ˜¯é˜¶è·ƒå‡½æ•°ã€‚

**å‡ ä½•ç›´è§‚**ï¼šæ„ŸçŸ¥æœºå­¦åˆ°çš„æ˜¯ä¸€æ¡**åˆ†å‰²çº¿**ï¼ˆæˆ–é«˜ç»´çš„è¶…å¹³é¢ï¼‰ã€‚

```
å¯¹äº 2D äºŒåˆ†ç±»é—®é¢˜ï¼š
    
    Class 1 â€¢â€¢â€¢
           â€¢â€¢â€¢
    â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” å†³ç­–è¾¹ç•Œï¼ˆç”± wÂ·x + b = 0 å®šä¹‰ï¼‰
           â€¢â€¢â€¢
    Class -1â€¢â€¢â€¢
```

**å­¦ä¹ ç®—æ³•ï¼ˆæ„ŸçŸ¥æœºè§„åˆ™ï¼‰**ï¼š

```
åˆå§‹åŒ–ï¼šw â† 0, b â† 0
é‡å¤ç›´åˆ°æ”¶æ•›ï¼š
    å¯¹æ¯ä¸ªæ ·æœ¬ (x_i, y_i):
        z = wÂ·x_i + b
        Å· = sign(z)
        å¦‚æœ Å· â‰  y_i:  # é¢„æµ‹é”™è¯¯
            w â† w + y_i Â· x_i
            b â† b + y_i
```

**ä¸ºä»€ä¹ˆæœ‰æ•ˆ**ï¼šæ¯å½“é¢„æµ‹é”™è¯¯æ—¶ï¼Œæˆ‘ä»¬æœç€æ­£ç¡®æ ‡ç­¾çš„æ–¹å‘è°ƒæ•´æƒé‡ã€‚

**å…³é”®é™åˆ¶**ï¼š
- âŒ åªèƒ½è§£å†³çº¿æ€§å¯åˆ†é—®é¢˜
- âŒ XOR é—®é¢˜æ— æ³•è§£å†³ï¼ˆç»å…¸çš„åä¾‹ï¼‰

```
XOR é—®é¢˜ï¼š
è¾“å…¥  | è¾“å‡º
-----+----
0, 0 | 0
0, 1 | 1
1, 0 | 1
1, 1 | 0

ä¸å­˜åœ¨ä¸€æ¡ç›´çº¿èƒ½åˆ†ç¦»è¿™äº›ç‚¹ï¼
```

#### 2. å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰â€”â€”æ·±åº¦å­¦ä¹ çš„åŸºç¡€

**çªç ´ç‚¹**ï¼šæ·»åŠ éšè—å±‚å’Œéçº¿æ€§æ¿€æ´»å‡½æ•°ï¼

**æ¶æ„**ï¼š

```
è¾“å…¥å±‚ x          éšè—å±‚ h          è¾“å‡ºå±‚ Å·
  xâ‚                hâ‚
  xâ‚‚ â”€â”€â”€â”€(Wâ‚,bâ‚)â”€â”€ hâ‚‚ â”€â”€â”€(Wâ‚‚,bâ‚‚)â”€â”€ Å·
  xâ‚ƒ                hâ‚ƒ
  (nç»´)     (mç»´)      (kç»´)
```

![MLP æ¶æ„è¯¦è§£](/img/deep-learning-math/07_mlp_architecture.webp)

**å‰å‘ä¼ æ’­**ï¼š

å±‚ 1ï¼š$\mathbf{z}^{(1)} = \mathbf{W}^{(1)} \mathbf{x} + \mathbf{b}^{(1)}$

æ¿€æ´»ï¼š$\mathbf{h} = \sigma(\mathbf{z}^{(1)})$

å±‚ 2ï¼š$\mathbf{z}^{(2)} = \mathbf{W}^{(2)} \mathbf{h} + \mathbf{b}^{(2)}$

è¾“å‡ºï¼š$\hat{\mathbf{y}} = f(\mathbf{z}^{(2)})$ï¼ˆå–å†³äºä»»åŠ¡ï¼Œåˆ†ç±»ç”¨ softmaxï¼Œå›å½’ç”¨æ’ç­‰ï¼‰

**é€šç”¨é€¼è¿‘å®šç†ï¼ˆUniversal Approximation Theoremï¼‰**ï¼š

**å®šç†**ï¼šåªè¦éšè—å±‚è¶³å¤Ÿå¤§ï¼Œå•éšè—å±‚çš„ MLP å¯ä»¥ä»¥ä»»æ„ç²¾åº¦é€¼è¿‘ä»»ä½•è¿ç»­å‡½æ•°ã€‚

**å…³é”®å‰æ**ï¼šå¿…é¡»ä½¿ç”¨**éçº¿æ€§æ¿€æ´»å‡½æ•°**ï¼

**ä¸ºä»€ä¹ˆï¼Ÿ** å¦‚æœæ²¡æœ‰éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå…¨æ˜¯çº¿æ€§å˜æ¢ï¼‰ï¼š

$$\mathbf{y} = \mathbf{W}^{(2)}(\mathbf{W}^{(1)}\mathbf{x} + \mathbf{b}^{(1)}) + \mathbf{b}^{(2)} = (\mathbf{W}^{(2)}\mathbf{W}^{(1)})\mathbf{x} + \ldots = \mathbf{W}_{\text{eff}}\mathbf{x} + b_{\text{eff}}$$

å¤šå±‚é€€åŒ–ä¸ºå•å±‚çº¿æ€§å˜æ¢ï¼æ— æ³•å­¦åˆ°å¤æ‚çš„éçº¿æ€§å…³ç³»ã€‚

**å®ä¾‹ï¼šè§£å†³ XOR é—®é¢˜**

![æ„ŸçŸ¥æœºä¸ XOR é—®é¢˜](/img/deep-learning-math/01_perceptron_vs_xor.webp)

```
MLP æ¶æ„ï¼š2è¾“å…¥ â†’ 2éšè— â†’ 1è¾“å‡º

å‰å‘ä¼ æ’­ï¼š
zâ‚ = Wâ‚Â·x + bâ‚                (2Ã—2 çŸ©é˜µ Ã— 2ç»´å‘é‡ = 2ç»´)
h = Ïƒ(zâ‚)                      (åº”ç”¨éçº¿æ€§æ¿€æ´»)
zâ‚‚ = Wâ‚‚Â·h + bâ‚‚                (1Ã—2 çŸ©é˜µ Ã— 2ç»´å‘é‡ = 1ç»´)
Å· = sigmoid(zâ‚‚)                (è¾“å‡ºæ¦‚ç‡)

å…³é”®ï¼šéšè—å±‚é€šè¿‡éçº¿æ€§æ¿€æ´»ï¼Œå°† 2D è¾“å…¥"å¼¯æ›²"å˜æ¢ï¼Œ
ä½¿å¾—åŸæœ¬çº¿æ€§ä¸å¯åˆ†çš„æ•°æ®åœ¨æ–°ç©ºé—´ä¸­å˜å¾—å¯åˆ†ã€‚
```

---

### æ¿€æ´»å‡½æ•°çš„éçº¿æ€§æœ¬è´¨

#### ä¸ºä»€ä¹ˆéœ€è¦æ¿€æ´»å‡½æ•°ï¼Ÿ

**é—®é¢˜**ï¼šå¦‚ä¸Šæ‰€è¿°ï¼Œæ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œæ·±å±‚ç½‘ç»œé€€åŒ–ä¸ºå•å±‚çº¿æ€§æ¨¡å‹ã€‚

**è§£å†³**ï¼šå¼•å…¥éçº¿æ€§æ¿€æ´»å‡½æ•° $\sigma(\cdot)$ã€‚

**å‡½æ•°è¦æ±‚**ï¼š
1. **éçº¿æ€§**ï¼š$\sigma(x) \neq ax + b$ï¼ˆé¿å…çº¿æ€§é€€åŒ–ï¼‰
2. **å¯å¯¼**ï¼šä¾¿äºåå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
3. **è®¡ç®—é«˜æ•ˆ**ï¼šè¦åœ¨æ•°ç™¾ä¸‡ä¸ªç¥ç»å…ƒä¸Šæ‰§è¡Œ
4. **æ¢¯åº¦æµé€š**ï¼šé¿å…æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸

#### ä¸»è¦æ¿€æ´»å‡½æ•°å¯¹æ¯”

![æ¿€æ´»å‡½æ•°å¯¹æ¯”è¯¦è§£](/img/deep-learning-math/02_activation_functions.webp)

**1. Sigmoid å‡½æ•°**

$$\sigma(x) = \frac{1}{1+e^{-x}}$$

$$\sigma'(x) = \sigma(x)(1-\sigma(x))$$

**æ€§è´¨**ï¼š
- è¾“å‡ºèŒƒå›´ï¼š$(0, 1)$ â€”â€” å¯ä»¥è§£é‡Šä¸ºæ¦‚ç‡
- å¯¼æ•°æœ€å¤§å€¼ï¼š0.25ï¼ˆå½“ $x=0$ æ—¶ï¼‰
- âœ… æ—©æœŸå¹¿æ³›ä½¿ç”¨ï¼ˆé€»è¾‘å›å½’ï¼‰
- âŒ æ˜“å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ï¼š$\sigma'(x) \leq 0.25$

**æ¢¯åº¦æ¶ˆå¤±æ¼”ç¤º**ï¼š

è®¾ç½‘ç»œæœ‰ 50 å±‚ï¼Œæ¯å±‚éƒ½ç”¨ Sigmoidï¼Œåˆ™ï¼š

$$\frac{\partial L}{\partial \mathbf{w}^{(1)}} \propto \prod_{l=1}^{50} \sigma'(z^{(l)}) \leq 0.25^{50} \approx 10^{-30}$$

æ¢¯åº¦å‡ ä¹ä¸ºé›¶ï¼Œç¬¬ä¸€å±‚å‡ ä¹æ— æ³•å­¦ä¹ ï¼

**2. Tanh å‡½æ•°**

$$\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = 2\sigma(2x) - 1$$

$$\tanh'(x) = 1 - \tanh^2(x)$$

**æ€§è´¨**ï¼š
- è¾“å‡ºèŒƒå›´ï¼š$(-1, 1)$ï¼ˆä»¥é›¶ä¸ºä¸­å¿ƒï¼Œæ¯” Sigmoid æ›´å¥½ï¼‰
- å¯¼æ•°æœ€å¤§å€¼ï¼š1.0ï¼ˆå½“ $x=0$ æ—¶ï¼‰
- âœ… æ¯” Sigmoid ç¨å¥½ï¼Œä½†ä»æœ‰æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- ğŸ“Š å¯¹ç§°æ€§ï¼š$\tanh(-x) = -\tanh(x)$

**å¯¹æ¯”**ï¼šTanh vs Sigmoid

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(-4, 4, 1000)
sigmoid = 1 / (1 + np.exp(-x))
tanh = np.tanh(x)

# Sigmoid è¾“å‡ºåœ¨ (0, 1)ï¼Œå¹³å‡å€¼ 0.5
# Tanh è¾“å‡ºåœ¨ (-1, 1)ï¼Œå¹³å‡å€¼ 0
# è¿™ä½¿å¾— Tanh çš„æ”¶æ•›æ›´å¿«
```

**3. ReLUï¼ˆRectified Linear Unitï¼‰**

$$\text{ReLU}(x) = \max(0, x)$$

$$\text{ReLU}'(x) = \begin{cases} 1 & x > 0 \\ 0 & x < 0 \end{cases}$$

**æ€§è´¨**ï¼š
- âœ… å¯¼æ•°ä¸º 1ï¼ˆä¸æ¶ˆå¤±ï¼‰æˆ– 0ï¼ˆç¨€ç–ï¼‰â€”â€” å®Œç¾è§£å†³æ¢¯åº¦æ¶ˆå¤±
- âœ… è®¡ç®—ç®€å•ï¼ˆåªéœ€ä¸€æ¬¡æ¯”è¾ƒï¼‰
- âœ… å¤©ç„¶ç¨€ç–æ€§ï¼ˆçº¦ 50% çš„ç¥ç»å…ƒè¾“å‡ºä¸º 0ï¼‰
- âŒ "æ­»äº¡ ReLU" é—®é¢˜ï¼š$x < 0$ æ—¶æ¢¯åº¦ä¸º 0ï¼Œè‹¥æƒé‡åˆå§‹åŒ–ä¸å½“ï¼Œç¥ç»å…ƒå¯èƒ½æ°¸ä¹…æ­»äº¡

```
æ­»äº¡ ReLU åœºæ™¯ï¼š
è‹¥æŸä¸ªç¥ç»å…ƒæ€»æ˜¯è¾“å‡ºè´Ÿæ•°ï¼Œå…¶æ¢¯åº¦æ°¸ä¸º 0ï¼Œæ— æ³•æ›´æ–°ã€‚
è§£å†³æ–¹æ¡ˆï¼š
  1. ä»”ç»†çš„æƒé‡åˆå§‹åŒ–ï¼ˆHe åˆå§‹åŒ–ï¼‰
  2. ä½¿ç”¨ Leaky ReLU
```

**4. Leaky ReLU**

$$\text{Leaky ReLU}(x) = \begin{cases} x & x > 0 \\ \alpha x & x \leq 0 \end{cases}$$

å…¶ä¸­ $\alpha$ æ˜¯å°çš„æ­£æ•°ï¼ˆé€šå¸¸ 0.01ï¼‰ã€‚

**å¯¼æ•°**ï¼š
$$\text{Leaky ReLU}'(x) = \begin{cases} 1 & x > 0 \\ \alpha & x \leq 0 \end{cases}$$

**ä¼˜ç‚¹**ï¼šé¿å…äº† ReLU çš„æ­»äº¡é—®é¢˜â€”â€”$x < 0$ æ—¶ä»æœ‰æ¢¯åº¦ $\alpha$ã€‚

**5. ELUï¼ˆExponential Linear Unitï¼‰**

$$\text{ELU}(x) = \begin{cases} x & x > 0 \\ \alpha(e^x - 1) & x \leq 0 \end{cases}$$

**ä¼˜ç‚¹**ï¼š
- è´Ÿè¾“å…¥æ—¶å¹³æ»‘ï¼ˆå¯¼æ•°è¿ç»­ï¼‰
- è¾“å‡ºæ¥è¿‘é›¶ä¸­å¿ƒ
- æ¢¯åº¦æµé€šå¥½

#### æ¿€æ´»å‡½æ•°çš„é€‰æ‹©æŒ‡å—

| æ¿€æ´»å‡½æ•° | æ¢¯åº¦æ¶ˆå¤± | è®¡ç®—é€Ÿåº¦ | ç¨€ç–æ€§ | æ¨èåœºæ™¯ |
|---------|---------|--------|-------|---------|
| Sigmoid | âŒ ä¸¥é‡ | å¿« | æ—  | è¾“å‡ºå±‚ï¼ˆäºŒåˆ†ç±»ï¼‰|
| Tanh | âŒ ä¸­ç­‰ | å¿« | æ—  | è¾“å‡ºå±‚ï¼ˆå›å½’ï¼‰|
| ReLU | âœ… æ—  | æœ€å¿« | âœ… é«˜ | éšè—å±‚ï¼ˆé¦–é€‰ï¼‰|
| Leaky ReLU | âœ… æ—  | å¿« | âœ… ä¸­ | éšè—å±‚ï¼ˆæ›¿ä»£ ReLUï¼‰|
| ELU | âœ… æ—  | è¾ƒæ…¢ | âœ… ä½ | éšè—å±‚ï¼ˆç²¾ç»†åº”ç”¨ï¼‰|

---

### æŸå¤±å‡½æ•°çš„é€‰æ‹©ä¸è®¾è®¡

æŸå¤±å‡½æ•°æ˜¯ä¼˜åŒ–çš„ç›®æ ‡ï¼Œä¸åŒä»»åŠ¡éœ€è¦ä¸åŒçš„æŸå¤±å‡½æ•°ã€‚

#### 1. MSEï¼ˆå‡æ–¹è¯¯å·®ï¼‰â€”â€” å›å½’ä»»åŠ¡

$$L = \text{MSE} = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2$$

**å®šä¹‰**ï¼šé¢„æµ‹å€¼ä¸çœŸå®å€¼å·®çš„å¹³æ–¹çš„å¹³å‡ã€‚

**æ¢¯åº¦**ï¼š
$$\frac{\partial L}{\partial \hat{y}_i} = -\frac{2}{n}(y_i - \hat{y}_i)$$

**æ€§è´¨**ï¼š
- âœ… ç›´è§‚ã€æ˜“è®¡ç®—
- âœ… å¯¹å¤§è¯¯å·®çš„æƒ©ç½šæ›´é‡ï¼ˆå¹³æ–¹é¡¹ï¼‰
- âŒ å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ
- âŒ å½“è¯¯å·®å¾ˆå¤§æ—¶ï¼Œæ¢¯åº¦ä¹Ÿå¾ˆå¤§ï¼Œå¯èƒ½å¯¼è‡´ä¸ç¨³å®š

**é€‚ç”¨åœºæ™¯**ï¼šè¿ç»­å€¼é¢„æµ‹ï¼ˆæˆ¿ä»·ã€æ¸©åº¦ã€è‚¡ç¥¨ä»·æ ¼ç­‰ï¼‰

**æ”¹è¿›ç‰ˆæœ¬ï¼šRMSEï¼ˆæ ¹å‡æ–¹è¯¯å·®ï¼‰**
$$\text{RMSE} = \sqrt{\frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2}$$
ä¼˜ç‚¹ï¼šé‡çº²ä¸ $y$ ç›¸åŒï¼Œæ˜“äºè§£é‡Šã€‚

#### 2. Cross Entropyï¼ˆäº¤å‰ç†µï¼‰â€”â€” åˆ†ç±»ä»»åŠ¡

**äºŒåˆ†ç±»**ï¼š

$$L = -\frac{1}{n}\sum_{i=1}^n [y_i \log \hat{y}_i + (1-y_i)\log(1-\hat{y}_i)]$$

å…¶ä¸­ $y_i \in \{0,1\}$ æ˜¯çœŸå®æ ‡ç­¾ï¼Œ$\hat{y}_i \in (0,1)$ æ˜¯é¢„æµ‹æ¦‚ç‡ã€‚

**å¤šåˆ†ç±»**ï¼š

$$L = -\frac{1}{n}\sum_{i=1}^n \sum_{c=1}^C y_{ic} \log \hat{y}_{ic}$$

å…¶ä¸­ $y_{ic}$ æ˜¯ç‹¬çƒ­ç¼–ç ï¼ˆåªæœ‰æ­£ç¡®ç±»ä¸º 1ï¼Œå…¶ä½™ä¸º 0ï¼‰ã€‚

**æ¢¯åº¦**ï¼ˆä½¿ç”¨ Softmax è¾“å‡ºæ—¶ï¼‰ï¼š

$$\frac{\partial L}{\partial z_k} = \hat{y}_k - y_k$$

è¿™æ˜¯ä¸€ä¸ª**éå¸¸ä¼˜ç¾çš„ç»“æœ**ï¼æ¢¯åº¦å°±æ˜¯é¢„æµ‹å’ŒçœŸå®çš„å·®ã€‚

**æ€§è´¨**ï¼š
- âœ… åŸºäºä¿¡æ¯è®ºï¼ˆKL æ•£åº¦ï¼‰
- âœ… æ¢¯åº¦æ¸…æ™°ï¼ˆä¸ä¼šå› ä¸ºç½‘ç»œå¼€å§‹çŠ¯é”™æ—¶æ¢¯åº¦å°±å¾ˆå°ï¼‰
- âœ… å¼ºåˆ¶è¾“å‡ºä¸ºæ¦‚ç‡åˆ†å¸ƒ
- âœ… å½“é¢„æµ‹é”™è¯¯æ—¶ï¼ŒæŸå¤±å¿«é€Ÿå¢é•¿ï¼ˆé¼“åŠ±ä¿®æ­£ï¼‰

**å¯¹æ¯”ï¼šMSE vs Cross Entropy åœ¨åˆ†ç±»ä¸­**

![æŸå¤±å‡½æ•°å¯¹æ¯”è¯¦è§£](/img/deep-learning-math/04_loss_functions.webp)

```python
å‡è®¾çœŸå®æ ‡ç­¾ y = [1, 0, 0]ï¼ˆç¬¬ä¸€ç±»ï¼‰
é¢„æµ‹ç»“æœ Å· = [0.8, 0.1, 0.1]ï¼ˆæ¥è¿‘æ­£ç¡®ï¼‰

MSE = 1/3 * [(1-0.8)Â² + (0-0.1)Â² + (0-0.1)Â²]
    = 1/3 * [0.04 + 0.01 + 0.01] = 0.0133

CrossEntropy = -[1Â·ln(0.8) + 0Â·ln(0.1) + 0Â·ln(0.1)]
             = -ln(0.8) â‰ˆ 0.223

ç°åœ¨å‡è®¾é¢„æµ‹å®Œå…¨é”™è¯¯ Å· = [0.1, 0.6, 0.3]ï¼š

MSE = 1/3 * [(1-0.1)Â² + (0-0.6)Â² + (0-0.3)Â²]
    = 1/3 * [0.81 + 0.36 + 0.09] = 0.42

CrossEntropy = -ln(0.1) â‰ˆ 2.303

å…³é”®è§‚å¯Ÿï¼š
- åœ¨ç¬¬äºŒä¸ªä¾‹å­ä¸­ï¼ŒCrossEntropy ä» 0.223 å¢é•¿åˆ° 2.303ï¼ˆå¢é•¿ 10 å€ï¼‰
- MSE ä» 0.0133 å¢é•¿åˆ° 0.42ï¼ˆå¢é•¿ 31 å€ï¼‰
- CrossEntropy çš„å¢é•¿æ›´"æ¸©å’Œ"ä¸”å¯é¢„æµ‹ï¼Œæ¢¯åº¦æ›´ç¨³å®š
```

#### 3. å…¶ä»–å¸¸è§æŸå¤±å‡½æ•°

**Huber Loss**ï¼ˆrobust regressionï¼‰ï¼š
$$L = \begin{cases} 
\frac{1}{2}e^2 & |e| \leq \delta \\
\delta|e| - \frac{1}{2}\delta^2 & |e| > \delta
\end{cases}$$

å…¶ä¸­ $e = y - \hat{y}$ æ˜¯è¯¯å·®ã€‚

**ä¼˜ç‚¹**ï¼šç»“åˆäº† MSEï¼ˆå¯¹å°è¯¯å·®ï¼‰å’Œ MAEï¼ˆå¯¹å¤§è¯¯å·®ï¼‰çš„ä¼˜ç‚¹ï¼Œå¯¹å¼‚å¸¸å€¼é²æ£’ã€‚

**Focal Loss**ï¼ˆå¤„ç†ç±»ä¸å¹³è¡¡ï¼‰ï¼š
$$L = -\alpha(1-p_t)^\gamma \log(p_t)$$

å…¶ä¸­ $p_t$ æ˜¯æ­£ç¡®ç±»çš„é¢„æµ‹æ¦‚ç‡ï¼Œ$\gamma$ æ˜¯èšç„¦å‚æ•°ã€‚

**ä¼˜ç‚¹**ï¼šè®©æ¨¡å‹æ›´ä¸“æ³¨äºéš¾ä»¥åˆ†ç±»çš„æ ·æœ¬ã€‚

---

### ä¼˜åŒ–ç®—æ³•åŸºç¡€

#### 1. æ¢¯åº¦ä¸‹é™ï¼ˆGradient Descentï¼‰â€”â€” æœ€åŸºç¡€çš„ä¼˜åŒ–

**ç®—æ³•**ï¼š

```
åˆå§‹åŒ–å‚æ•° Î¸
å­¦ä¹ ç‡ Î±ï¼ˆè¶…å‚æ•°ï¼‰

é‡å¤ç›´åˆ°æ”¶æ•›ï¼š
    è®¡ç®—æ¢¯åº¦ g = âˆ‡L(Î¸)
    æ›´æ–°å‚æ•° Î¸ â† Î¸ - Î±Â·g
```

**å‡ ä½•æ„ä¹‰**ï¼šæ¯æ¬¡æ²¿æ¢¯åº¦åæ–¹å‘ï¼ˆä¸‹é™æœ€å¿«ï¼‰èµ°ä¸€å°æ­¥ã€‚

**æ”¶æ•›é€Ÿåº¦åˆ†æ**ï¼š

å¯¹äºå‡¸å‡½æ•°ï¼Œè®¾ Lipschitz å¸¸æ•°ä¸º $\beta$ï¼Œåˆ™ï¼š
- å¦‚æœå­¦ä¹ ç‡ $\alpha < \frac{2}{\beta}$ï¼Œæ¢¯åº¦ä¸‹é™æ”¶æ•›
- æ”¶æ•›é€Ÿåº¦ï¼š$O(1/t)$ï¼ˆ$t$ æ˜¯è¿­ä»£æ•°ï¼‰

**é—®é¢˜**ï¼š
- âŒ éœ€è¦è®¡ç®—å…¨æ•°æ®é›†çš„æ¢¯åº¦ï¼ˆæ…¢ï¼‰
- âŒ å®¹æ˜“é™·å…¥å±€éƒ¨æœ€å°å€¼
- âŒ åœ¨éç‚¹ï¼ˆsaddle pointï¼‰å¯èƒ½å¡ä½

#### 2. éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰â€”â€” å®è·µä¸­çš„æ ‡å‡†

**æ”¹è¿›**ï¼šä½¿ç”¨å°æ‰¹é‡ï¼ˆmini-batchï¼‰æ¢¯åº¦è€Œéå…¨æ‰¹é‡ï¼š

```
åˆå§‹åŒ–å‚æ•° Î¸
å­¦ä¹ ç‡ Î±
æ‰¹é‡å¤§å° B

é‡å¤ç›´åˆ°æ”¶æ•›ï¼š
    éšæœºé‡‡æ ·ä¸€ä¸ªå°æ‰¹é‡ {(xâ‚,yâ‚),...,(x_B,y_B)}
    è®¡ç®—å°æ‰¹é‡æ¢¯åº¦ g = 1/B âˆ‘áµ¢ âˆ‡L(Î¸; xáµ¢, yáµ¢)
    æ›´æ–°å‚æ•° Î¸ â† Î¸ - Î±Â·g
```

**ä¼˜åŠ¿**ï¼š
- âœ… è®¡ç®—å¿«ï¼ˆåªéœ€å°æ‰¹é‡ï¼‰
- âœ… å¤©ç„¶çš„æ­£åˆ™åŒ–æ•ˆæœï¼ˆå™ªå£°å¸®åŠ©é€ƒç¦»å±€éƒ¨æœ€å°å€¼ï¼‰
- âœ… å†…å­˜é«˜æ•ˆ
- âœ… å¯å¤„ç†æµæ•°æ®

**æ”¶æ•›æ€§**ï¼ˆéšæœºæƒ…å†µä¸‹ï¼‰ï¼š

è™½ç„¶å•æ­¥å™ªå£°å¤§ï¼Œä½†å¤§æ•°å®šå¾‹ä¿è¯ï¼š
$$\mathbb{E}[g_{\text{mini-batch}}] = \nabla L(\theta)$$

å¹³å‡è€Œè¨€ï¼Œæ¢¯åº¦æ–¹å‘æ˜¯æ­£ç¡®çš„ã€‚

**å­¦ä¹ ç‡çš„å½±å“**ï¼š

![å­¦ä¹ ç‡æ•ˆæœåˆ†æ](/img/deep-learning-math/05_learning_rate_effect.webp)

```
å­¦ä¹ ç‡å¤ªå°ï¼ˆÎ± = 0.001ï¼‰ï¼š
- æ”¶æ•›å¾ˆæ…¢
- éœ€è¦å¾ˆå¤šè¿­ä»£
- ä½†æœ€ç»ˆèƒ½æ”¶æ•›
- ç±»ä¼¼çˆ¬å±±çˆ¬å¾—å¾ˆæ…¢

å­¦ä¹ ç‡å¤ªå¤§ï¼ˆÎ± = 0.1ï¼‰ï¼š
- å¯èƒ½ç›´æ¥è·³è¿‡æœ€å°å€¼
- æŸå¤±æ³¢åŠ¨å‰§çƒˆ
- å¯èƒ½å‘æ•£
- ç±»ä¼¼å¤§æ­¥è¿ˆï¼Œå®¹æ˜“æ‘”è·¤

æœ€ä¼˜å­¦ä¹ ç‡ï¼ˆÎ± = 0.01ï¼‰ï¼š
- å¿«é€Ÿæ”¶æ•›
- æŸå¤±å¹³ç¨³ä¸‹é™
```

---

### åå‘ä¼ æ’­ç®—æ³•è¯¦è§£

**è¿™æ˜¯æ·±åº¦å­¦ä¹ çš„çµé­‚ã€‚** åå‘ä¼ æ’­å°±æ˜¯é“¾å¼æ³•åˆ™çš„å·§å¦™åº”ç”¨ã€‚

![åå‘ä¼ æ’­è¿‡ç¨‹å®Œæ•´æ¼”ç¤º](/img/deep-learning-math/06_sgd_convergence.webp)

#### æ ¸å¿ƒæ€æƒ³

ç»™å®šè®¡ç®—å›¾ï¼Œåå‘ä¼ æ’­æŒ‰æ‹“æ‰‘é¡ºåºçš„åå‘ï¼ˆä»è¾“å‡ºåˆ°è¾“å…¥ï¼‰è®¡ç®—æ¢¯åº¦ã€‚

#### å®Œæ•´ä¾‹å­ï¼šä¸€ä¸ªå°ç¥ç»ç½‘ç»œ

**ç½‘ç»œç»“æ„**ï¼š
```
è¾“å…¥ xâ‚, xâ‚‚
    â†“
  zâ‚ = wâ‚xâ‚ + wâ‚‚xâ‚‚ + bâ‚
    â†“
  h = ReLU(zâ‚)  (æ¿€æ´»ï¼Œå¼•å…¥éçº¿æ€§)
    â†“
  zâ‚‚ = wâ‚ƒh + bâ‚‚
    â†“
  Å· = sigmoid(zâ‚‚)  (è¾“å‡ºæ¦‚ç‡)
    â†“
  L = -yÂ·ln(Å·) - (1-y)Â·ln(1-Å·)  (äºŒå…ƒäº¤å‰ç†µ)
```

**å‰å‘ä¼ æ’­ï¼ˆè®¡ç®—æŸå¤±ï¼‰**ï¼š

ç»™å®š $x_1 = 2, x_2 = 3, y = 1$ï¼ˆçœŸå®æ ‡ç­¾ï¼‰
åˆå§‹å‚æ•° $w_1=0.5, w_2=0.3, w_3=0.4, b_1=0.1, b_2=0.2$

```
Step 1: zâ‚ = 0.5Ã—2 + 0.3Ã—3 + 0.1 = 1.0 + 0.9 + 0.1 = 2.0
Step 2: h = ReLU(2.0) = 2.0  (å› ä¸º > 0)
Step 3: zâ‚‚ = 0.4Ã—2.0 + 0.2 = 0.8 + 0.2 = 1.0
Step 4: Å· = sigmoid(1.0) = 1/(1+eâ»Â¹) â‰ˆ 0.731
Step 5: L = -1Â·ln(0.731) - 0Â·ln(0.269) â‰ˆ 0.314
```

**åå‘ä¼ æ’­ï¼ˆè®¡ç®—æ¢¯åº¦ï¼‰**ï¼š

ä»æŸå¤±å¼€å§‹ï¼Œé€å±‚åå‘è®¡ç®—æ¢¯åº¦ã€‚

```
Step 1: æŸå¤±å¯¹è¾“å‡ºçš„æ¢¯åº¦
dL/dÅ· = -y/Å· + (1-y)/(1-Å·)
      = -1/0.731 + 0/0.269
      â‰ˆ -1.368

Step 2: è¾“å‡º sigmoid çš„å¯¼æ•°
dÅ·/dzâ‚‚ = Å·(1-Å·) = 0.731 Ã— 0.269 â‰ˆ 0.197
(sigmoid çš„å¯¼æ•°)

Step 3: é“¾å¼æ³•åˆ™
dL/dzâ‚‚ = dL/dÅ· Ã— dÅ·/dzâ‚‚ = -1.368 Ã— 0.197 â‰ˆ -0.269

Step 4: zâ‚‚ å¯¹å‚æ•°å’Œæ¿€æ´»çš„æ¢¯åº¦
dL/dwâ‚ƒ = dL/dzâ‚‚ Ã— dzâ‚‚/dwâ‚ƒ = -0.269 Ã— h = -0.269 Ã— 2.0 â‰ˆ -0.538
dL/dbâ‚‚ = dL/dzâ‚‚ Ã— dzâ‚‚/dbâ‚‚ = -0.269 Ã— 1 â‰ˆ -0.269

Step 5: åå‘é€šè¿‡æ¿€æ´»å‡½æ•°ï¼ˆReLUï¼‰
dL/dh = dL/dzâ‚‚ Ã— dzâ‚‚/dh = -0.269 Ã— wâ‚ƒ = -0.269 Ã— 0.4 â‰ˆ -0.108
ç”±äº h = ReLU(zâ‚) ä¸” zâ‚ > 0:
dL/dzâ‚ = dL/dh Ã— dh/dzâ‚ = -0.108 Ã— 1 â‰ˆ -0.108

Step 6: ç¬¬ä¸€å±‚å‚æ•°æ¢¯åº¦
dL/dwâ‚ = dL/dzâ‚ Ã— dzâ‚/dwâ‚ = -0.108 Ã— xâ‚ = -0.108 Ã— 2 â‰ˆ -0.216
dL/dwâ‚‚ = dL/dzâ‚ Ã— dzâ‚/dwâ‚‚ = -0.108 Ã— xâ‚‚ = -0.108 Ã— 3 â‰ˆ -0.324
dL/dbâ‚ = dL/dzâ‚ Ã— dzâ‚/dbâ‚ = -0.108 Ã— 1 â‰ˆ -0.108
```

**å‚æ•°æ›´æ–°**ï¼ˆå­¦ä¹ ç‡ Î± = 0.1ï¼‰ï¼š

```
wâ‚ â† wâ‚ - Î±Â·dL/dwâ‚ = 0.5 - 0.1Ã—(-0.216) = 0.5 + 0.0216 = 0.5216
wâ‚‚ â† wâ‚‚ - Î±Â·dL/dwâ‚‚ = 0.3 - 0.1Ã—(-0.324) = 0.3 + 0.0324 = 0.3324
wâ‚ƒ â† wâ‚ƒ - Î±Â·dL/dwâ‚ƒ = 0.4 - 0.1Ã—(-0.538) = 0.4 + 0.0538 = 0.4538
bâ‚ â† bâ‚ - Î±Â·dL/dbâ‚ = 0.1 - 0.1Ã—(-0.108) = 0.1 + 0.0108 = 0.1108
bâ‚‚ â† bâ‚‚ - Î±Â·dL/dbâ‚‚ = 0.2 - 0.1Ã—(-0.269) = 0.2 + 0.0269 = 0.2269
```

æ‰€æœ‰å‚æ•°éƒ½è¢«æ›´æ–°äº†ï¼Œä¸”ç”±äºæ¢¯åº¦ä¸ºè´Ÿï¼Œæ›´æ–°å¢åŠ äº†å‚æ•°å€¼ã€‚è¿™æ ·ä¼šå‡å°‘æŸå¤±ï¼ˆå‘åæ–¹å‘ï¼‰ã€‚

#### åå‘ä¼ æ’­çš„çŸ©é˜µå½¢å¼

å¯¹äºä¸€èˆ¬çš„ç½‘ç»œå±‚ï¼š

```
å‰å‘ï¼šz^(l) = W^(l)Â·a^(l-1) + b^(l)
      a^(l) = Ïƒ(z^(l))

åå‘ï¼ˆå…³é”®é€’æ¨å…³ç³»ï¼‰ï¼š
      Î´^(l) = (W^(l+1)áµ€Â·Î´^(l+1)) âŠ™ Ïƒ'(z^(l))
      
å‚æ•°æ¢¯åº¦ï¼š
      dL/dW^(l) = Î´^(l)Â·(a^(l-1))áµ€
      dL/db^(l) = Î´^(l)
```

å…¶ä¸­ï¼š
- $\delta^{(l)} = \frac{\partial L}{\partial z^{(l)}}$ æ˜¯æŸå¤±å¯¹é¢„æ¿€æ´»å€¼çš„æ¢¯åº¦
- $âŠ™$ æ˜¯é€å…ƒç´ ä¹˜ç§¯ï¼ˆHadamard ç§¯ï¼‰
- $Ïƒ'(z^{(l)})$ æ˜¯æ¿€æ´»å‡½æ•°çš„å¯¼æ•°

#### åå‘ä¼ æ’­çš„è®¡ç®—å¤æ‚æ€§

**å‰å‘ä¼ æ’­**ï¼š$O(æ€»æƒé‡æ•°)$ 

**åå‘ä¼ æ’­**ï¼š$O(æ€»æƒé‡æ•°)$ â€”â€” å‡ ä¹ç›¸åŒï¼

è¿™å°±æ˜¯åå‘ä¼ æ’­é«˜æ•ˆçš„åŸå› ï¼šå®ƒä¸å‰å‘ä¼ æ’­æœ‰ç›¸åŒçš„å¤æ‚åº¦ï¼Œä½†è®¡ç®—äº†æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ã€‚

#### å®ç°ç»†èŠ‚ï¼šæ•°å€¼ç¨³å®šæ€§

**é—®é¢˜**ï¼šæ¢¯åº¦å¯èƒ½éå¸¸å°æˆ–éå¸¸å¤§

**è§£å†³æ–¹æ¡ˆ 1ï¼šæ¢¯åº¦å½’ä¸€åŒ–**
```python
if ||gradient|| > threshold:
    gradient = gradient / ||gradient|| * threshold
```

**è§£å†³æ–¹æ¡ˆ 2ï¼šæ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰**
```python
gradient = np.clip(gradient, -clip_value, clip_value)
```

**è§£å†³æ–¹æ¡ˆ 3ï¼šæ‰¹é‡å½’ä¸€åŒ–**
```
å½’ä¸€åŒ–æ¯å±‚çš„è¾“å…¥ï¼ˆåç»­è¯¦ç»†è®²è§£ï¼‰
```

---

### å°ç»“ï¼šç®—æ³•å±‚çš„å®Œæ•´æµç¨‹

```
1. å‰å‘ä¼ æ’­ï¼šè¾“å…¥ â†’ è®¡ç®—æ¯å±‚æ¿€æ´» â†’ è®¡ç®—æŸå¤±
2. åå‘ä¼ æ’­ï¼šæŸå¤± â†’ é“¾å¼æ³•åˆ™ â†’ è®¡ç®—æ‰€æœ‰å‚æ•°æ¢¯åº¦
3. å‚æ•°æ›´æ–°ï¼šä½¿ç”¨æ¢¯åº¦ä¸‹é™ï¼ˆæˆ–å˜ç§å¦‚ Adamï¼‰æ›´æ–°å‚æ•°
4. é‡å¤ï¼šå¾ªç¯å¤šä¸ª epoch ç›´åˆ°æ”¶æ•›
```

**å…³é”®æ•°å­¦å¼•ç†**ï¼š

| æ­¥éª¤ | æ•°å­¦ | è®¡ç®—æ•ˆç‡ |
|------|------|---------|
| å‰å‘ | $a^{(l)} = \sigma(W^{(l)}a^{(l-1)} + b^{(l)})$ | $O(N)$ï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰ |
| åå‘ | $\delta^{(l)} = (W^{(l+1)T}\delta^{(l+1)})âŠ™\sigma'(z^{(l)})$ | $O(N)$ï¼ˆåŒæ ·æ“ä½œï¼‰|
| æ›´æ–° | $\theta â† \theta - Î±âˆ‡L$ | $O(N)$ï¼ˆé€å…ƒç´ ï¼‰ |

è¿™ä¸‰ä¸ªæ­¥éª¤éƒ½æ˜¯çº¿æ€§å¤æ‚åº¦ï¼Œå› æ­¤æ€»æ—¶é—´å¤æ‚åº¦ä¸º $O(N)$ï¼Œå…¶ä¸­ $N$ æ˜¯å‚æ•°æ€»æ•°ã€‚

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šå¾®ç§¯åˆ†åŸºç¡€

### 1.1 å¯¼æ•°çš„ä¸¥æ ¼å®šä¹‰

**å®šä¹‰**ï¼šå‡½æ•° $f: \mathbb{R} \to \mathbb{R}$ åœ¨ç‚¹ $x_0$ å¤„çš„å¯¼æ•°å®šä¹‰ä¸ºï¼š

$$f'(x_0) = \lim_{h \to 0} \frac{f(x_0 + h) - f(x_0)}{h}$$

**æ·±å±‚å«ä¹‰**ï¼š
- å¯¼æ•°è¡¡é‡å‡½æ•°åœ¨è¯¥ç‚¹çš„**ç¬æ—¶å˜åŒ–ç‡**
- å‡ ä½•æ„ä¹‰æ˜¯æ›²çº¿åœ¨è¯¥ç‚¹çš„**åˆ‡çº¿æ–œç‡**
- åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œ$\nabla f$ æŒ‡å‘æŸå¤±å‡½æ•°å¢é•¿æœ€å¿«çš„æ–¹å‘

**åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„ä½“ç°**ï¼š
å¦‚æœæˆ‘ä»¬çš„æŸå¤±å‡½æ•°æ˜¯ $L(\theta)$ï¼Œå…¶ä¸­ $\theta$ æ˜¯æ¨¡å‹å‚æ•°ï¼Œé‚£ä¹ˆ $\frac{\partial L}{\partial \theta}$ å‘Šè¯‰æˆ‘ä»¬æ”¹å˜å‚æ•°å¦‚ä½•å½±å“æŸå¤±ã€‚

### 1.2 å¤šå…ƒå‡½æ•°çš„åå¯¼æ•°

å¯¹äºå‡½æ•° $f: \mathbb{R}^n \to \mathbb{R}$ï¼Œåå¯¼æ•°å®šä¹‰ä¸ºï¼š

$$\frac{\partial f}{\partial x_i} = \lim_{h \to 0} \frac{f(x_1, \ldots, x_i + h, \ldots, x_n) - f(x_1, \ldots, x_n)}{h}$$

**æ¢¯åº¦å‘é‡**ï¼ˆæ¢¯åº¦æ˜¯æ‰€æœ‰åå¯¼æ•°çš„é›†åˆï¼‰ï¼š

$$\nabla f = \begin{pmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{pmatrix}$$

**æ€§è´¨**ï¼š
- æ¢¯åº¦ $\nabla f$ æ°¸è¿œæŒ‡å‘å‡½æ•°å¢é•¿æœ€å¿«çš„æ–¹å‘
- æ¢¯åº¦çš„æ¨¡ $\|\nabla f\|$ è¡¨ç¤ºå¢é•¿çš„é€Ÿç‡
- è´Ÿæ¢¯åº¦ $-\nabla f$ æŒ‡å‘å‡½æ•°ä¸‹é™æœ€å¿«çš„æ–¹å‘

**åœ¨ç¥ç»ç½‘ç»œä¸­**ï¼šä¸€ä¸ªç®€å•çš„ 2 å±‚ç½‘ç»œæœ‰æ•°ç™¾ä¸‡ä¸ªå‚æ•°ï¼Œæ¢¯åº¦å°±æ˜¯è¿™å‡ ç™¾ä¸‡ä¸ªåå¯¼æ•°ç»„æˆçš„å‘é‡ã€‚

### 1.3 Jacobian çŸ©é˜µä¸ Hessian çŸ©é˜µ

**Jacobian çŸ©é˜µ**ï¼šå¯¹äºå‘é‡å‡½æ•° $\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m$ï¼š

$$J = \begin{pmatrix} 
\frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n}
\end{pmatrix}$$

**æ„ä¹‰**ï¼šåœ¨ç¥ç»ç½‘ç»œçš„åå‘ä¼ æ’­ä¸­ï¼ŒJacobian çŸ©é˜µæè¿°äº†æ¯ä¸€å±‚è¾“å‡ºå¯¹è¾“å…¥çš„å¯¼æ•°ã€‚

**Hessian çŸ©é˜µ**ï¼šå¯¹äºæ ‡é‡å‡½æ•° $f: \mathbb{R}^n \to \mathbb{R}$ï¼ŒäºŒé˜¶å¯¼æ•°çŸ©é˜µï¼š

$$H = \begin{pmatrix} 
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{pmatrix}$$

**æ„ä¹‰**ï¼šHessian çŸ©é˜µæè¿°äº†æŸå¤±å‡½æ•°çš„æ›²ç‡ï¼Œå¯¹äºç†è§£äºŒé˜¶ä¼˜åŒ–æ–¹æ³•ï¼ˆå¦‚ Newton æ–¹æ³•ï¼‰è‡³å…³é‡è¦ã€‚

### 1.4 æ–¹å‘å¯¼æ•°ä¸æ¢¯åº¦çš„å…³ç³»

å¯¹äºå•ä½æ–¹å‘å‘é‡ $\mathbf{u}$ï¼Œæ–¹å‘å¯¼æ•°ä¸ºï¼š

$$D_{\mathbf{u}}f = \nabla f \cdot \mathbf{u} = \|\nabla f\| \cos(\theta)$$

å…¶ä¸­ $\theta$ æ˜¯æ¢¯åº¦ä¸æ–¹å‘å‘é‡çš„å¤¹è§’ã€‚

**å…³é”®æ´å¯Ÿ**ï¼š
- å½“ $\theta = 0$ æ—¶ï¼ˆæ²¿æ¢¯åº¦æ–¹å‘ï¼‰ï¼Œæ–¹å‘å¯¼æ•°æœ€å¤§ï¼Œå‡½æ•°å¢é•¿æœ€å¿«
- å½“ $\theta = 180Â°$ æ—¶ï¼ˆæ²¿è´Ÿæ¢¯åº¦æ–¹å‘ï¼‰ï¼Œå‡½æ•°ä¸‹é™æœ€å¿«
- è¿™æ­£æ˜¯æ¢¯åº¦ä¸‹é™ç®—æ³•çš„ç†è®ºåŸºç¡€ï¼

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šé“¾å¼æ³•åˆ™ä¸åå‘ä¼ æ’­

### 2.1 ä¸€ç»´æƒ…å†µä¸‹çš„é“¾å¼æ³•åˆ™

**åŸºç¡€å½¢å¼**ï¼šè®¾ $y = f(g(x))$ï¼Œåˆ™ï¼š

$$\frac{dy}{dx} = \frac{dy}{dg} \cdot \frac{dg}{dx}$$

**å…·ä½“ä¾‹å­**ï¼š
$$y = \sin(x^2)$$

ä½¿ç”¨é“¾å¼æ³•åˆ™ï¼š
$$\frac{dy}{dx} = \cos(x^2) \cdot 2x$$

### 2.2 å¤šå…ƒå‡½æ•°çš„é“¾å¼æ³•åˆ™ï¼ˆçŸ©é˜µå½¢å¼ï¼‰

è®¾ $\mathbf{y} = f(g(\mathbf{x}))$ï¼Œå…¶ä¸­ï¼š
- $\mathbf{x} \in \mathbb{R}^n$
- $\mathbf{g}: \mathbb{R}^n \to \mathbb{R}^m$
- $f: \mathbb{R}^m \to \mathbb{R}$

åˆ™åå¯¼æ•°æ»¡è¶³é“¾å¼æ³•åˆ™ï¼š

$$\frac{\partial f}{\partial x_i} = \sum_{j=1}^{m} \frac{\partial f}{\partial g_j} \cdot \frac{\partial g_j}{\partial x_i}$$

ç”¨çŸ©é˜µè¡¨ç¤ºï¼š

$$\nabla_{\mathbf{x}} f = J_g^T \nabla_{\mathbf{g}} f$$

å…¶ä¸­ $J_g$ æ˜¯ $g$ çš„ Jacobian çŸ©é˜µã€‚

### 2.3 è®¡ç®—å›¾ä¸è‡ªåŠ¨å¾®åˆ†

**è®¡ç®—å›¾**æ˜¯è¡¨ç¤ºå‡½æ•°å¤åˆçš„æœ‰å‘æ— ç¯å›¾ã€‚æ¯ä¸ªèŠ‚ç‚¹è¡¨ç¤ºä¸€ä¸ªæ“ä½œæˆ–å˜é‡ï¼Œè¾¹è¡¨ç¤ºæ•°æ®æµã€‚

**ä¾‹å­**ï¼šè®¡ç®— $z = (x + y) \cdot (y + 1)$

```
        x       y
         \     /
          (add)
           |
           u â€”â€”â€”â€”â€”â€”â”
                   |
              y -- (add)
                   |
                   v
                   |
                 (mul)
                   |
                   z
```

**æ­£å‘ä¼ æ’­ï¼ˆForward Passï¼‰**ï¼š
$$u = x + y = 2 + 3 = 5$$
$$v = y + 1 = 3 + 1 = 4$$
$$z = u \cdot v = 5 \cdot 4 = 20$$

**åå‘ä¼ æ’­ï¼ˆBackward Passï¼‰**ï¼Œè®¡ç®— $\frac{\partial z}{\partial x}$, $\frac{\partial z}{\partial y}$ï¼š

1. åˆå§‹åŒ–ï¼š$\frac{\partial z}{\partial z} = 1$

2. ä¹˜æ³•èŠ‚ç‚¹ï¼š
   - $\frac{\partial z}{\partial u} = v = 4$
   - $\frac{\partial z}{\partial v} = u = 5$

3. åŠ æ³•èŠ‚ç‚¹ï¼ˆå·¦è¾¹ï¼‰ï¼š
   - $\frac{\partial z}{\partial x} = \frac{\partial z}{\partial u} \cdot \frac{\partial u}{\partial x} = 4 \cdot 1 = 4$
   - $\frac{\partial z}{\partial y} = \frac{\partial z}{\partial u} \cdot \frac{\partial u}{\partial y} = 4 \cdot 1 = 4$

4. åŠ æ³•èŠ‚ç‚¹ï¼ˆå³è¾¹ï¼‰ï¼š
   - $\frac{\partial z}{\partial y} \text{(from right)} = \frac{\partial z}{\partial v} \cdot \frac{\partial v}{\partial y} = 5 \cdot 1 = 5$

5. ç´¯åŠ ï¼š$\frac{\partial z}{\partial y} = 4 + 5 = 9$

**æœ€ç»ˆç»“æœ**ï¼š$\frac{\partial z}{\partial x} = 4$ï¼Œ$\frac{\partial z}{\partial y} = 9$

### 2.4 ç¥ç»ç½‘ç»œä¸­çš„åå‘ä¼ æ’­

å¯¹äºä¸€ä¸ª $L$ å±‚çš„ç¥ç»ç½‘ç»œï¼š

$$\mathbf{z}^{(l)} = \mathbf{W}^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}$$
$$\mathbf{a}^{(l)} = \sigma(\mathbf{z}^{(l)})$$

å…¶ä¸­ $\sigma$ æ˜¯æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ ReLUã€Sigmoid ç­‰ï¼‰ã€‚

**åå‘ä¼ æ’­çš„æ ¸å¿ƒé€’æ¨å…³ç³»**ï¼š

$$\delta^{(l)} = (\mathbf{W}^{(l+1)T} \delta^{(l+1)}) \odot \sigma'(\mathbf{z}^{(l)})$$

è¿™é‡Œï¼š
- $\delta^{(l)} = \frac{\partial L}{\partial \mathbf{z}^{(l)}}$ æ˜¯æŸå¤±å¯¹ç¬¬ $l$ å±‚é¢„æ¿€æ´»å€¼çš„æ¢¯åº¦
- $\odot$ è¡¨ç¤ºé€å…ƒç´ ä¹˜ç§¯ï¼ˆHadamard ç§¯ï¼‰
- $\sigma'$ æ˜¯æ¿€æ´»å‡½æ•°çš„å¯¼æ•°

**å‚æ•°æ¢¯åº¦**ï¼š

$$\frac{\partial L}{\partial \mathbf{W}^{(l)}} = \delta^{(l)} \mathbf{a}^{(l-1)T}$$
$$\frac{\partial L}{\partial \mathbf{b}^{(l)}} = \delta^{(l)}$$

**å…³é”®æ´å¯Ÿ**ï¼šåå‘ä¼ æ’­æœ¬è´¨ä¸Šå°±æ˜¯é“¾å¼æ³•åˆ™çš„é€’å½’åº”ç”¨ï¼æ¯ä¸€å±‚è®¡ç®—è‡ªå·±çš„æ¢¯åº¦æ—¶ï¼Œéƒ½ä½¿ç”¨ä¸Šä¸€å±‚å·²ç»è®¡ç®—å¥½çš„æ¢¯åº¦ã€‚

### 2.5 æ¢¯åº¦æ¶ˆå¤±ä¸æ¢¯åº¦çˆ†ç‚¸é—®é¢˜

**æ¢¯åº¦æ¶ˆå¤±é—®é¢˜**ï¼š

å½“ç½‘ç»œå¾ˆæ·±æ—¶ï¼Œç”±äºé“¾å¼æ³•åˆ™ä¸­æœ‰è¿ç»­çš„ä¹˜æ³•ï¼š

$$\frac{\partial L}{\partial \mathbf{W}^{(1)}} = \frac{\partial L}{\partial \mathbf{a}^{(L)}} \prod_{l=2}^{L} \frac{\partial \mathbf{a}^{(l)}}{\partial \mathbf{z}^{(l)}} \frac{\partial \mathbf{z}^{(l)}}{\partial \mathbf{a}^{(l-1)}}$$

å¦‚æœæ¯ä¸ª $\frac{\partial \mathbf{a}^{(l)}}{\partial \mathbf{z}^{(l)}}$ éƒ½å°äº 1ï¼ˆå¦‚ä½¿ç”¨ Sigmoid å‡½æ•°ï¼Œå…¶å¯¼æ•°æœ€å¤§ä¸º 0.25ï¼‰ï¼Œåˆ™ä¹˜ç§¯ä¼šä»¥æŒ‡æ•°é€Ÿåº¦è¡°å‡ã€‚

**æ•°å­¦è¡¨è¿°**ï¼šè®¾ $\sigma'(z) \leq 0.25$ï¼Œåˆ™ï¼š

$$\left\|\frac{\partial L}{\partial \mathbf{W}^{(1)}}\right\| \leq 0.25^L \left\|\frac{\partial L}{\partial \mathbf{a}^{(L)}}\right\|$$

å½“ $L = 50$ æ—¶ï¼Œ$0.25^{50} \approx 10^{-30}$ï¼Œæ¢¯åº¦å‡ ä¹å®Œå…¨æ¶ˆå¤±ï¼

![æ¢¯åº¦æ¶ˆå¤±é—®é¢˜æ¼”ç¤º](/img/deep-learning-math/03_gradient_vanishing.webp)

**è§£å†³æ–¹æ¡ˆ**ï¼š
- ä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°ï¼ˆå¯¼æ•°ä¸º 0 æˆ– 1ï¼Œä¸ä¼šè¡°å‡ï¼‰
- æ‰¹é‡å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰
- æ®‹å·®è¿æ¥ï¼ˆResidual Connectionsï¼‰

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ¢¯åº¦ä¸‹é™ä¸ä¼˜åŒ–

### 3.1 æ¢¯åº¦ä¸‹é™çš„å‡ ä½•ç›´è§‚

æ¢¯åº¦ä¸‹é™ç®—æ³•çš„æ›´æ–°è§„åˆ™ï¼š

$$\theta_{t+1} = \theta_t - \alpha \nabla L(\theta_t)$$

å…¶ä¸­ $\alpha$ æ˜¯å­¦ä¹ ç‡ã€‚

**å‡ ä½•æ„ä¹‰**ï¼šæ¯æ¬¡è¿­ä»£ï¼Œæˆ‘ä»¬æ²¿ç€æŸå¤±å‡½æ•°æ¢¯åº¦çš„åæ–¹å‘ç§»åŠ¨ $\alpha$ çš„è·ç¦»ã€‚

### 3.2 æ”¶æ•›æ€§åˆ†æ

**å®šç†ï¼ˆGradient Descent æ”¶æ•›æ€§ï¼‰**ï¼šå‡è®¾ $L(\theta)$ æ˜¯ $\beta$-å…‰æ»‘çš„ï¼ˆå³ Lipschitz è¿ç»­æ¢¯åº¦ï¼‰ï¼Œ$\alpha < \frac{2}{\beta}$ï¼Œåˆ™æ¢¯åº¦ä¸‹é™æ”¶æ•›åˆ°ä¸´ç•Œç‚¹ã€‚

**å…‰æ»‘æ€§çš„å®šä¹‰**ï¼šå‡½æ•° $L$ æ˜¯ $\beta$-å…‰æ»‘çš„ï¼Œå½“ä¸”ä»…å½“ï¼š

$$\|\nabla L(\theta_1) - \nabla L(\theta_2)\| \leq \beta \|\theta_1 - \theta_2\|$$

**æ”¶æ•›ç‡**ï¼šå¯¹äºå¼ºå‡¸å‡½æ•°ï¼Œæ”¶æ•›ç‡ä¸ºæŒ‡æ•°çº§çš„ï¼ˆçº¿æ€§æ”¶æ•›ï¼‰ï¼š

$$L(\theta_t) - L(\theta^*) \leq (1 - \frac{2\alpha \mu}{\beta})^t (L(\theta_0) - L(\theta^*))$$

å…¶ä¸­ $\mu$ æ˜¯å¼ºå‡¸ç³»æ•°ï¼Œ$\theta^*$ æ˜¯æœ€ä¼˜è§£ã€‚

### 3.3 éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰

ç”±äºç¥ç»ç½‘ç»œé€šå¸¸åœ¨å¤§æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œæˆ‘ä»¬ä½¿ç”¨ SGD è€Œéå…¨é‡æ¢¯åº¦ä¸‹é™ï¼š

$$\theta_{t+1} = \theta_t - \alpha \nabla L_{\text{batch}}(\theta_t)$$

å…¶ä¸­ $L_{\text{batch}} = \frac{1}{B} \sum_{i \in \text{batch}} L_i(\theta_t)$ æ˜¯ä¸€ä¸ªéšæœºå°æ‰¹é‡ä¸Šçš„æŸå¤±ã€‚

**éšæœºæ€§çš„ä½œç”¨**ï¼š
- å‡å°‘å†…å­˜å ç”¨
- å¼•å…¥å™ªå£°ï¼Œæœ‰åŠ©äºé€ƒç¦»å±€éƒ¨æœ€å°å€¼
- å¤©ç„¶çš„æ­£åˆ™åŒ–æ•ˆæœ

### 3.4 åŠ¨é‡æ–¹æ³•ï¼ˆMomentumï¼‰

åŸºç¡€æ¢¯åº¦ä¸‹é™çš„é—®é¢˜ï¼šåœ¨"ä¹‹å­—å½¢"çš„å±±è°·ä¸­æŒ¯è¡å‰§çƒˆã€‚

**å¸¦åŠ¨é‡çš„æ›´æ–°**ï¼š

$$\mathbf{v}_{t+1} = \gamma \mathbf{v}_t + \nabla L(\theta_t)$$
$$\theta_{t+1} = \theta_t - \alpha \mathbf{v}_{t+1}$$

å…¶ä¸­ $\gamma \in (0, 1)$ æ˜¯åŠ¨é‡ç³»æ•°ï¼Œé€šå¸¸å– 0.9ã€‚

**æ•°å­¦ç›´è§‚**ï¼šå¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªä½é€šæ»¤æ³¢å™¨ï¼ŒæŠ‘åˆ¶é«˜é¢‘æŒ¯è¡ï¼Œä¿ç•™ä½é¢‘è¶‹åŠ¿ã€‚

**åŠ æƒå¹³å‡è§£é‡Š**ï¼š

$$\mathbf{v}_{t+1} = \sum_{j=0}^{t} \gamma^j \nabla L(\theta_{t-j})$$

å†å²æ¢¯åº¦çš„æƒé‡å‘ˆæŒ‡æ•°è¡°å‡ï¼Œæœ€è¿‘çš„æ¢¯åº¦æƒé‡æœ€å¤§ã€‚

### 3.5 Adam ä¼˜åŒ–å™¨çš„å®Œæ•´æ•°å­¦æ¨å¯¼

Adamï¼ˆAdaptive Moment Estimationï¼‰ç»“åˆäº† RMSprop å’Œ Momentum çš„æ€æƒ³ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼šç»´æŠ¤æ¢¯åº¦çš„ä¸€é˜¶çŸ©ï¼ˆå‡å€¼ï¼‰å’ŒäºŒé˜¶çŸ©ï¼ˆæ–¹å·®ï¼‰çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ã€‚

**æ›´æ–°è§„åˆ™**ï¼š

$$m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t$$
$$v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$$

å…¶ä¸­ $g_t = \nabla L(\theta_t)$ æ˜¯æ—¶é—´ $t$ çš„æ¢¯åº¦ã€‚

**åå·®æ ¡æ­£**ï¼š
ç”±äº $m_0 = 0$ï¼Œ$v_0 = 0$ï¼ŒåˆæœŸä¼šä¸¥é‡å‘é›¶åç§»ã€‚å› æ­¤éœ€è¦æ ¡æ­£ï¼š

$$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$$
$$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$$

**å‚æ•°æ›´æ–°**ï¼š

$$\theta_{t+1} = \theta_t - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$

å…¶ä¸­ $\epsilon$ æ˜¯ä¸€ä¸ªå°å¸¸æ•°ï¼ˆé€šå¸¸ $10^{-8}$ï¼‰é˜²æ­¢é™¤ä»¥é›¶ã€‚

**ç›´è§‚ç†è§£**ï¼š
- åˆ†å­ $\hat{m}_t$ æ˜¯ä¸€ä¸ª"åŠ æƒç§»åŠ¨å¹³å‡"çš„æ¢¯åº¦ï¼Œæä¾›æ–¹å‘æ€§
- åˆ†æ¯ $\sqrt{\hat{v}_t}$ æ˜¯æ¢¯åº¦å¹…åº¦çš„è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œå¯¹æ¢¯åº¦å¤§çš„å‚æ•°é™ä½å­¦ä¹ ç‡

---

## ç¬¬å››éƒ¨åˆ†ï¼šæŸå¤±å‡½æ•°ä¸ä¿¡æ¯è®º

### 4.1 äº¤å‰ç†µæŸå¤±çš„æ¨å¯¼

åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒçœŸå®æ ‡ç­¾æ˜¯æ¦‚ç‡åˆ†å¸ƒ $p(x)$ï¼Œæ¨¡å‹é¢„æµ‹æ˜¯ $q(x)$ã€‚

**KL æ•£åº¦**ï¼ˆç›¸å¯¹ç†µï¼‰ï¼šè¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„å·®å¼‚

$$D_{KL}(p \| q) = \sum_x p(x) \log \frac{p(x)}{q(x)} = H(p, q) - H(p)$$

å…¶ä¸­ï¼š
- $H(p, q) = -\sum_x p(x) \log q(x)$ æ˜¯**äº¤å‰ç†µ**
- $H(p) = -\sum_x p(x) \log p(x)$ æ˜¯ $p$ çš„ç†µ

ç”±äº $H(p)$ æ˜¯å¸¸æ•°ï¼ˆçœŸå®åˆ†å¸ƒå›ºå®šï¼‰ï¼Œæœ€å°åŒ– KL æ•£åº¦ç­‰ä»·äºæœ€å°åŒ–äº¤å‰ç†µï¼š

$$L = H(p, q) = -\sum_{c=1}^C y_c \log \hat{y}_c$$

å…¶ä¸­ $y_c$ æ˜¯ç¬¬ $c$ ç±»çš„çœŸå®æ ‡ç­¾ï¼ˆç‹¬çƒ­ç¼–ç ï¼‰ï¼Œ$\hat{y}_c$ æ˜¯æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡ã€‚

### 4.2 Softmax å‡½æ•°ä¸æ¢¯åº¦è®¡ç®—

å¯¹äºå¤šåˆ†ç±»é—®é¢˜ï¼Œè¾“å‡ºå±‚ä½¿ç”¨ softmax å‡½æ•°ï¼š

$$\hat{y}_i = \frac{e^{z_i}}{\sum_{j=1}^C e^{z_j}}$$

å…¶ä¸­ $z_i$ æ˜¯ç¬¬ $i$ ä¸ªç±»çš„è¾“å‡ºå±‚é¢„æ¿€æ´»å€¼ã€‚

**æ¢¯åº¦è®¡ç®—**ï¼ˆå¯¹æ•°æ®æµéå¸¸é‡è¦ï¼‰ï¼š

$$\frac{\partial L}{\partial z_i} = \hat{y}_i - y_i$$

**æ¨å¯¼**ï¼ˆç®€åŒ–å½¢å¼ï¼‰ï¼š

$$\frac{\partial L}{\partial z_i} = -\sum_{c=1}^C y_c \frac{\partial \log \hat{y}_c}{\partial z_i}$$

ç”±äº $y$ æ˜¯ç‹¬çƒ­ç¼–ç ï¼Œåªæœ‰çœŸå®ç±» $k$ å¯¹åº”çš„ $y_k = 1$ï¼š

$$\frac{\partial L}{\partial z_i} = -\frac{\partial \log \hat{y}_k}{\partial z_i}$$

ä½¿ç”¨é“¾å¼æ³•åˆ™å’Œ softmax çš„å¯¼æ•°æ€§è´¨ï¼š

$$\frac{\partial \log \hat{y}_k}{\partial z_i} = \frac{\partial \log \hat{y}_k}{\partial \hat{y}_k} \cdot \frac{\partial \hat{y}_k}{\partial z_i} = \frac{1}{\hat{y}_k} \cdot \frac{\partial \hat{y}_k}{\partial z_i}$$

ç»è¿‡å¤æ‚çš„è®¡ç®—å¯å¾—ï¼š

$$\frac{\partial L}{\partial z_i} = \hat{y}_i - y_i$$

**å…³é”®ç‰¹æ€§**ï¼šæ¢¯åº¦æ˜¯é¢„æµ‹æ¦‚ç‡ä¸çœŸå®æ ‡ç­¾çš„å·®ï¼è¿™åœ¨ç›´è§‚ä¸Šä¹Ÿå¾ˆåˆç†ï¼šé”™è¯¯è¶Šå¤§ï¼Œæ¢¯åº¦è¶Šå¤§ã€‚

### 4.3 L2 æ­£åˆ™åŒ–çš„è´å¶æ–¯è§£é‡Š

L2 æ­£åˆ™åŒ–æŸå¤±ï¼š

$$L_{\text{total}} = L(y, \hat{y}) + \lambda \|\theta\|_2^2$$

ä»è´å¶æ–¯è§’åº¦ï¼šå‡è®¾å‚æ•°æœä»é«˜æ–¯å…ˆéªŒ $p(\theta) = \mathcal{N}(0, \sigma^2)$ï¼Œåˆ™ï¼š

$$-\log p(\theta | D) = -\log p(D | \theta) - \log p(\theta) + \text{const}$$

å…¶ä¸­ï¼š
- $-\log p(D | \theta) \propto L(y, \hat{y})$ æ˜¯æ•°æ®ä¼¼ç„¶çš„è´Ÿå¯¹æ•°
- $-\log p(\theta) = \frac{\|\theta\|_2^2}{2\sigma^2}$ æ˜¯é«˜æ–¯å…ˆéªŒçš„è´Ÿå¯¹æ•°

å› æ­¤ï¼Œæœ€å¤§åéªŒæ¦‚ç‡ï¼ˆMAPï¼‰ä¼°è®¡ç­‰ä»·äºï¼š

$$\min_\theta L(y, \hat{y}) + \frac{1}{2\sigma^2} \|\theta\|_2^2$$

è¿™æ­£æ˜¯ L2 æ­£åˆ™åŒ–ï¼å‚æ•° $\lambda = \frac{1}{2\sigma^2}$ åæ˜ äº†æˆ‘ä»¬å¯¹å‚æ•°å¤§å°çš„å…ˆéªŒä¿¡å¿µã€‚

---

## ç¬¬äº”éƒ¨åˆ†ï¼šå®é™…æ¡ˆä¾‹â€”â€”ä»ç†è®ºåˆ°ä»£ç 

### 5.1 æ¡ˆä¾‹ï¼šå›¾åƒåˆ†ç±»ä¸­çš„æ•°å­¦åº”ç”¨

**èƒŒæ™¯**ï¼šä½¿ç”¨ CNN å¯¹ CIFAR-10 æ•°æ®é›†è¿›è¡Œåˆ†ç±»ã€‚

**æ¨¡å‹æ¶æ„**ï¼š
```
è¾“å…¥ (32Ã—32Ã—3) 
  â†’ Conv(3â†’32, kernel=3) 
  â†’ ReLU 
  â†’ MaxPool(2Ã—2)
  â†’ Conv(32â†’64, kernel=3) 
  â†’ ReLU 
  â†’ MaxPool(2Ã—2)
  â†’ Flatten 
  â†’ Dense(64â†’128)
  â†’ ReLU
  â†’ Dense(128â†’10)
  â†’ Softmax
```

### 5.2 å·ç§¯å±‚çš„æ•°å­¦è¡¨è¿°

**å·ç§¯æ“ä½œ**ï¼ˆå•ä¸ªè¾“å‡ºï¼‰ï¼š

$$y[i,j] = \sum_{m=0}^{K-1} \sum_{n=0}^{K-1} w[m,n] \cdot x[i+m, j+n] + b$$

å…¶ä¸­ $K$ æ˜¯å·ç§¯æ ¸å¤§å°ï¼Œ$w$ æ˜¯å·ç§¯æ ¸å‚æ•°ï¼Œ$b$ æ˜¯åç½®ã€‚

**å®Œæ•´å½¢å¼**ï¼ˆå¯¹æ‰€æœ‰è¾“å‡ºé€šé“ï¼‰ï¼š

$$y^{(l)}[i,j,c_{\text{out}}] = \sum_{c_{\text{in}}=1}^{C_{\text{in}}} \sum_{m=0}^{K-1} \sum_{n=0}^{K-1} w^{(l)}[m,n,c_{\text{in}},c_{\text{out}}] \cdot x^{(l-1)}[i+m, j+n, c_{\text{in}}] + b^{(l)}[c_{\text{out}}]$$

**å‚æ•°æ•°é‡**ï¼š$K \times K \times C_{\text{in}} \times C_{\text{out}}$

å¯¹äº $3 \times 3$ å·ç§¯ä» 32 é€šé“åˆ° 64 é€šé“ï¼š$3 \times 3 \times 32 \times 64 = 18432$ ä¸ªå‚æ•°ã€‚

### 5.3 åå‘ä¼ æ’­è¯¦ç»†è¿‡ç¨‹

**å‰å‘ä¼ æ’­æµç¨‹**ï¼š

```
è¾“å…¥ x
  â†“
zâ‚ = Conv(x)           â† 32 ä¸ªç‰¹å¾å›¾ï¼Œæ¯ä¸ª 30Ã—30
  â†“
aâ‚ = ReLU(zâ‚)          â† ReLU æ¿€æ´»
  â†“
pâ‚ = MaxPool(aâ‚)       â† æœ€å¤§æ± åŒ–ï¼Œ15Ã—15
  â†“
zâ‚‚ = Conv(pâ‚)          â† 64 ä¸ªç‰¹å¾å›¾ï¼Œ13Ã—13
  â†“
aâ‚‚ = ReLU(zâ‚‚)
  â†“
pâ‚‚ = MaxPool(aâ‚‚)       â† æœ€å¤§æ± åŒ–ï¼Œ6Ã—6
  â†“
f = Flatten(pâ‚‚)        â† å±•å¹³ï¼š6Ã—6Ã—64 = 2304 ç»´
  â†“
h = Dense(f)           â† 128 ç»´éšè—å±‚
  â†“
zâ‚ƒ = ReLU(h)
  â†“
logits = Dense(zâ‚ƒ)     â† 10 ç»´ç±»åˆ«å¾—åˆ†
  â†“
probs = Softmax(logits) â† æ¦‚ç‡åˆ†å¸ƒ
  â†“
L = CrossEntropy(probs, y_true)  â† æŸå¤±
```

**åå‘ä¼ æ’­è¯¦ç»†è®¡ç®—**ï¼š

1. **è¾“å‡ºå±‚æ¢¯åº¦**ï¼ˆå·²åœ¨ 4.2 èŠ‚æ¨å¯¼ï¼‰ï¼š
   $$\delta_{\text{logits}} = \text{probs} - y_{\text{true}}$$

2. **å€’æ•°ç¬¬äºŒå±‚æ¢¯åº¦**ï¼ˆä½¿ç”¨é“¾å¼æ³•åˆ™ï¼‰ï¼š
   $$\delta_h = \delta_{\text{logits}} \cdot W_{\text{Dense}}^T$$
   $$\delta_{z_3} = \delta_h \odot \mathbb{1}_{h > 0}$$  ï¼ˆReLU çš„å¯¼æ•°ï¼‰

3. **å±•å¹³å±‚åå‘**ï¼ˆé‡å¡‘æ¢¯åº¦ï¼‰ï¼š
   $$\delta_{p_2} = \text{reshape}(\delta_{z_3}, \text{shape}(p_2))$$

4. **æœ€å¤§æ± åŒ–åå‘**ï¼ˆåªæœ‰åœ¨æ­£å‘ä¼ æ’­ä¸­æ˜¯æœ€å¤§å€¼çš„ä½ç½®æœ‰æ¢¯åº¦ï¼‰ï¼š
   $$\delta_{a_2}[i,j] = \begin{cases} 
   \delta_{p_2}[i',j'] & \text{if } (i,j) = \arg\max \\
   0 & \text{otherwise}
   \end{cases}$$

5. **ç¬¬äºŒå·ç§¯å±‚åå‘**ï¼ˆéœ€è¦é€šè¿‡å·ç§¯æ±‚æ¢¯åº¦ï¼Œæ¶‰åŠ"è½¬ç½®å·ç§¯"ï¼‰ï¼š
   
   å¯¹äºå·ç§¯å‚æ•°çš„æ¢¯åº¦ï¼š
   $$\frac{\partial L}{\partial w^{(2)}} = \text{ConvTranspose}(p_1, \delta_{a_2})$$
   
   å¯¹äºè¾“å…¥çš„æ¢¯åº¦ï¼ˆä¼ æ’­åˆ°å‰ä¸€å±‚ï¼‰ï¼š
   $$\delta_{p_1} = \text{ConvTranspose}(w^{(2)}, \delta_{a_2})$$

### 5.4 å®Œæ•´çš„ Python å®ç°

```python
import numpy as np
from scipy import signal

class ConvLayer:
    """å·ç§¯å±‚çš„å®Œæ•´å®ç°ï¼Œå±•ç¤ºæ•°å­¦ç»†èŠ‚"""
    
    def __init__(self, in_channels, out_channels, kernel_size=3):
        """
        åˆå§‹åŒ–å·ç§¯å±‚å‚æ•°
        ä½¿ç”¨ He åˆå§‹åŒ–ç¡®ä¿æ¢¯åº¦çš„é€‚å½“ç¼©æ”¾
        """
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        
        # He åˆå§‹åŒ–ï¼šæ–¹å·®ä¸º 2/(fan_in)
        fan_in = in_channels * kernel_size * kernel_size
        stddev = np.sqrt(2.0 / fan_in)
        
        # å½¢çŠ¶ï¼š(kernel_size, kernel_size, in_channels, out_channels)
        self.W = np.random.randn(
            kernel_size, kernel_size, in_channels, out_channels
        ) * stddev
        self.b = np.zeros(out_channels)
        
        # ä¿å­˜ä¸­é—´å€¼ç”¨äºåå‘ä¼ æ’­
        self.cache = {}
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼šx çš„å½¢çŠ¶ä¸º (batch_size, height, width, channels)
        """
        batch_size, height, width, _ = x.shape
        
        # è¾“å‡ºå¤§å°ï¼ˆå‡è®¾ padding=0, stride=1ï¼‰
        out_height = height - self.kernel_size + 1
        out_width = width - self.kernel_size + 1
        
        output = np.zeros((
            batch_size, out_height, out_width, self.out_channels
        ))
        
        # å¯¹æ¯ä¸ªæ ·æœ¬ã€æ¯ä¸ªè¾“å‡ºä½ç½®ã€æ¯ä¸ªè¾“å‡ºé€šé“æ‰§è¡Œå·ç§¯
        for b in range(batch_size):
            for out_c in range(self.out_channels):
                for i in range(out_height):
                    for j in range(out_width):
                        # æå– patch
                        patch = x[b, i:i+self.kernel_size, j:j+self.kernel_size, :]
                        
                        # é€é€šé“å·ç§¯å’Œæ±‚å’Œ
                        conv_sum = 0
                        for in_c in range(self.in_channels):
                            conv_sum += np.sum(
                                patch[:, :, in_c] * self.W[:, :, in_c, out_c]
                            )
                        
                        output[b, i, j, out_c] = conv_sum + self.b[out_c]
        
        self.cache['x'] = x
        return output
    
    def backward(self, dL_dout):
        """
        åå‘ä¼ æ’­ï¼šè®¡ç®—æŸå¤±å¯¹å‚æ•°å’Œè¾“å…¥çš„æ¢¯åº¦
        
        dL_dout å½¢çŠ¶ï¼š(batch_size, out_height, out_width, out_channels)
        """
        batch_size, out_height, out_width, out_channels = dL_dout.shape
        x = self.cache['x']
        _, height, width, in_channels = x.shape
        
        # åˆå§‹åŒ–æ¢¯åº¦
        dL_dW = np.zeros_like(self.W)
        dL_db = np.zeros_like(self.b)
        dL_dx = np.zeros_like(x)
        
        # è®¡ç®—å‚æ•°æ¢¯åº¦å’Œè¾“å…¥æ¢¯åº¦
        for b in range(batch_size):
            for out_c in range(out_channels):
                for i in range(out_height):
                    for j in range(out_width):
                        # æå– patch
                        patch = x[b, i:i+self.kernel_size, j:j+self.kernel_size, :]
                        grad = dL_dout[b, i, j, out_c]
                        
                        # å‚æ•°æ¢¯åº¦ï¼šå±€éƒ¨æ¢¯åº¦ Ã— è¾“å…¥ patch
                        for in_c in range(in_channels):
                            dL_dW[:, :, in_c, out_c] += grad * patch[:, :, in_c]
                        
                        # è¾“å…¥æ¢¯åº¦ï¼šå±€éƒ¨æ¢¯åº¦ Ã— æƒé‡
                        dL_dx[b, i:i+self.kernel_size, j:j+self.kernel_size, :] += (
                            grad * self.W[:, :, :, out_c]
                        )
                
                # åç½®æ¢¯åº¦ï¼šå¯¹æ‰€æœ‰ç©ºé—´ä½ç½®æ±‚å’Œ
                dL_db[out_c] = np.sum(dL_dout[:, :, :, out_c])
        
        return dL_dx, dL_dW, dL_db
    
    def update(self, dL_dW, dL_db, learning_rate=0.01):
        """ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°"""
        self.W -= learning_rate * dL_dW
        self.b -= learning_rate * dL_db


class DenseLayer:
    """å…¨è¿æ¥å±‚"""
    
    def __init__(self, in_features, out_features):
        """åˆå§‹åŒ–å‚æ•°"""
        stddev = np.sqrt(2.0 / in_features)  # He åˆå§‹åŒ–
        self.W = np.random.randn(in_features, out_features) * stddev
        self.b = np.zeros(out_features)
        self.cache = {}
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        # x å½¢çŠ¶ï¼š(batch_size, in_features)
        z = np.dot(x, self.W) + self.b
        self.cache['x'] = x
        return z
    
    def backward(self, dL_dz):
        """
        åå‘ä¼ æ’­
        
        è¿™æ˜¯ä¸€ä¸ªæœ€æ¸…æ™°çš„çº¿æ€§å˜æ¢æ¢¯åº¦ä¾‹å­ï¼š
        å¦‚æœ z = xÂ·W + bï¼Œåˆ™
        dL/dW = x^T Â· dL/dz ï¼ˆå¤–ç§¯ï¼‰
        dL/dx = dL/dz Â· W^T
        """
        x = self.cache['x']
        batch_size = x.shape[0]
        
        # å‚æ•°æ¢¯åº¦
        dL_dW = np.dot(x.T, dL_dz) / batch_size
        dL_db = np.sum(dL_dz, axis=0) / batch_size
        
        # è¾“å…¥æ¢¯åº¦ï¼ˆä¼ æ’­åˆ°å‰ä¸€å±‚ï¼‰
        dL_dx = np.dot(dL_dz, self.W.T)
        
        return dL_dx, dL_dW, dL_db
    
    def update(self, dL_dW, dL_db, learning_rate=0.01):
        """å‚æ•°æ›´æ–°"""
        self.W -= learning_rate * dL_dW
        self.b -= learning_rate * dL_db


class ReLULayer:
    """ReLU æ¿€æ´»å‡½æ•°"""
    
    def __init__(self):
        self.cache = {}
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼ša = max(0, x)
        """
        self.cache['x'] = x
        return np.maximum(0, x)
    
    def backward(self, dL_da):
        """
        åå‘ä¼ æ’­ï¼š
        dL/dx = dL/da Â· da/dx
        å…¶ä¸­ da/dx = 1 if x > 0 else 0
        """
        x = self.cache['x']
        return dL_da * (x > 0).astype(float)


class SoftmaxCrossEntropyLoss:
    """
    Softmax + CrossEntropy ç»„åˆå±‚
    
    è¿™æ˜¯æ•°å€¼ç¨³å®šçš„å®ç°ï¼ˆé¿å…æº¢å‡ºï¼‰
    """
    
    def forward(self, logits, labels):
        """
        è¾“å…¥ï¼š
          logitsï¼šåŸå§‹è¾“å‡ºï¼Œå½¢çŠ¶ (batch_size, num_classes)
          labelsï¼šç‹¬çƒ­ç¼–ç æ ‡ç­¾ï¼Œå½¢çŠ¶ (batch_size, num_classes)
        
        è¿”å›ï¼šæŸå¤±å€¼
        """
        batch_size = logits.shape[0]
        
        # æ•°å€¼ç¨³å®šçš„ softmaxï¼šå‡å»æœ€å¤§å€¼
        max_logits = np.max(logits, axis=1, keepdims=True)
        exp_logits = np.exp(logits - max_logits)
        probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)
        
        # äº¤å‰ç†µæŸå¤±
        loss = -np.sum(labels * np.log(probs + 1e-8)) / batch_size
        
        self.cache = {'probs': probs, 'labels': labels}
        return loss
    
    def backward(self):
        """
        è¿”å›æŸå¤±å¯¹ logits çš„æ¢¯åº¦
        
        è¿™æ­£æ˜¯æˆ‘ä»¬åœ¨ 4.2 èŠ‚æ¨å¯¼çš„ä¼˜ç¾ç»“æœï¼š
        dL/dlogits = probs - labels
        """
        probs = self.cache['probs']
        labels = self.cache['labels']
        return (probs - labels) / labels.shape[0]


# ============ å®Œæ•´çš„è®­ç»ƒå¾ªç¯ ============

class SimpleConvNet:
    """ç®€åŒ–çš„ CNN ç”¨äºæ¼”ç¤ºæ•°å­¦åŸç†"""
    
    def __init__(self):
        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        self.conv1 = ConvLayer(in_channels=3, out_channels=32, kernel_size=3)
        self.relu1 = ReLULayer()
        
        # ç¬¬äºŒä¸ªå·ç§¯å—ï¼ˆçœç•¥ MaxPool ä»¥ç®€åŒ–ï¼‰
        self.conv2 = ConvLayer(in_channels=32, out_channels=64, kernel_size=3)
        self.relu2 = ReLULayer()
        
        # å…¨è¿æ¥å±‚
        # å‡è®¾ç»è¿‡å·ç§¯åçš„ç‰¹å¾æ˜¯ (64, 26, 26)
        self.dense1 = DenseLayer(in_features=64*26*26, out_features=128)
        self.relu3 = ReLULayer()
        
        self.dense2 = DenseLayer(in_features=128, out_features=10)
        
        # æŸå¤±å‡½æ•°
        self.loss_fn = SoftmaxCrossEntropyLoss()
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        z1 = self.conv1.forward(x)
        a1 = self.relu1.forward(z1)
        
        # ç¬¬äºŒä¸ªå·ç§¯å—
        z2 = self.conv2.forward(a1)
        a2 = self.relu2.forward(z2)
        
        # å±•å¹³
        batch_size = a2.shape[0]
        a2_flat = a2.reshape(batch_size, -1)
        
        # å…¨è¿æ¥å±‚
        z3 = self.dense1.forward(a2_flat)
        a3 = self.relu3.forward(z3)
        
        logits = self.dense2.forward(a3)
        
        return logits
    
    def backward(self, logits, labels):
        """åå‘ä¼ æ’­"""
        # è®¡ç®—æŸå¤±
        loss = self.loss_fn.forward(logits, labels)
        
        # ä»æŸå¤±å¼€å§‹çš„æ¢¯åº¦
        dL_dlogits = self.loss_fn.backward()
        
        # é€šè¿‡ dense2 åå‘ä¼ æ’­
        dL_da3, dL_dW2, dL_db2 = self.dense2.backward(dL_dlogits)
        
        # é€šè¿‡ relu3 åå‘ä¼ æ’­
        dL_dz3 = self.relu3.backward(dL_da3)
        
        # é€šè¿‡ dense1 åå‘ä¼ æ’­
        dL_da2_flat, dL_dW1, dL_db1 = self.dense1.backward(dL_dz3)
        
        # é‡å¡‘ä¸ºå·ç§¯è¾“å‡ºçš„å½¢çŠ¶
        batch_size = dL_da2_flat.shape[0]
        dL_da2 = dL_da2_flat.reshape(batch_size, 26, 26, 64)
        
        # é€šè¿‡ relu2 åå‘ä¼ æ’­
        dL_dz2 = self.relu2.backward(dL_da2)
        
        # é€šè¿‡ conv2 åå‘ä¼ æ’­
        dL_da1, dL_dW_conv2, dL_db_conv2 = self.conv2.backward(dL_dz2)
        
        # é€šè¿‡ relu1 åå‘ä¼ æ’­
        dL_dz1 = self.relu1.backward(dL_da1)
        
        # é€šè¿‡ conv1 åå‘ä¼ æ’­
        dL_dx, dL_dW_conv1, dL_db_conv1 = self.conv1.backward(dL_dz1)
        
        return loss, {
            'conv1': (dL_dW_conv1, dL_db_conv1),
            'conv2': (dL_dW_conv2, dL_db_conv2),
            'dense1': (dL_dW1, dL_db1),
            'dense2': (dL_dW2, dL_db2)
        }
    
    def update_parameters(self, gradients, learning_rate=0.01):
        """ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°"""
        self.conv1.update(*gradients['conv1'], learning_rate)
        self.conv2.update(*gradients['conv2'], learning_rate)
        self.dense1.update(*gradients['dense1'], learning_rate)
        self.dense2.update(*gradients['dense2'], learning_rate)
    
    def train_step(self, x_batch, y_batch, learning_rate=0.01):
        """å•ä¸ªè®­ç»ƒæ­¥éª¤"""
        # å‰å‘ä¼ æ’­
        logits = self.forward(x_batch)
        
        # åå‘ä¼ æ’­
        loss, gradients = self.backward(logits, y_batch)
        
        # å‚æ•°æ›´æ–°
        self.update_parameters(gradients, learning_rate)
        
        return loss


# ============ è®­ç»ƒç¤ºä¾‹ ============

if __name__ == "__main__":
    # åˆ›å»ºæ¨¡å‹
    model = SimpleConvNet()
    
    # æ¨¡æ‹Ÿæ•°æ®ï¼ˆå®é™…ä¸­åº”ä½¿ç”¨çœŸå®çš„ CIFAR-10 æ•°æ®ï¼‰
    batch_size = 4
    x_batch = np.random.randn(batch_size, 32, 32, 3) * 0.1  # å°å€¼ç¡®ä¿æ•°å€¼ç¨³å®š
    
    # åˆ›å»ºéšæœºæ ‡ç­¾ï¼ˆç‹¬çƒ­ç¼–ç ï¼‰
    y_batch = np.zeros((batch_size, 10))
    for i in range(batch_size):
        y_batch[i, np.random.randint(10)] = 1
    
    print("è®­ç»ƒå¼€å§‹...")
    for epoch in range(5):
        loss = model.train_step(x_batch, y_batch, learning_rate=0.01)
        print(f"Epoch {epoch+1}, Loss: {loss:.6f}")
    
    print("\nè®­ç»ƒå®Œæˆï¼")
    print("è¿™ä¸ªä¾‹å­å±•ç¤ºäº†ä»æ•°å­¦ç†è®ºåˆ°ä»£ç å®ç°çš„å®Œæ•´è¿‡ç¨‹ï¼š")
    print("1. å‰å‘ä¼ æ’­ï¼šæ¯å±‚è®¡ç®— z = Wx + b")
    print("2. æ¿€æ´»å‡½æ•°ï¼šåº”ç”¨éçº¿æ€§å˜æ¢")
    print("3. æŸå¤±è®¡ç®—ï¼šäº¤å‰ç†µæŸå¤±ï¼ˆSoftmax + äº¤å‰ç†µï¼‰")
    print("4. åå‘ä¼ æ’­ï¼šé“¾å¼æ³•åˆ™é€å±‚è®¡ç®—æ¢¯åº¦")
    print("5. å‚æ•°æ›´æ–°ï¼šæ¢¯åº¦ä¸‹é™ä¼˜åŒ–")
```

### 5.5 æ•°å­¦æ´å¯Ÿä¸å®ç°ç»†èŠ‚

#### å…³é”®æ•°å­¦ç‚¹ 1ï¼šæ¢¯åº¦çš„æ•°å€¼éªŒè¯

åœ¨å®ç°åå‘ä¼ æ’­æ—¶ï¼Œå¯ä»¥ä½¿ç”¨**æ•°å€¼æ¢¯åº¦æ£€éªŒ**éªŒè¯æ•°å­¦æ¨å¯¼çš„æ­£ç¡®æ€§ï¼š

```python
def numerical_gradient(f, x, h=1e-5):
    """è®¡ç®—æ•°å€¼æ¢¯åº¦ï¼ˆç”¨äºéªŒè¯ï¼‰"""
    grad = np.zeros_like(x)
    it = np.nditer(x, flags=['multi_index'])
    
    while not it.finished:
        idx = it.multi_index
        old_value = x[idx]
        
        # f(x + h)
        x[idx] = old_value + h
        fxh_pos = f(x)
        
        # f(x - h)
        x[idx] = old_value - h
        fxh_neg = f(x)
        
        # ä¸­å¿ƒå·®åˆ†
        grad[idx] = (fxh_pos - fxh_neg) / (2 * h)
        x[idx] = old_value
        
        it.iternext()
    
    return grad

def check_gradient(layer, x, dL_dout):
    """éªŒè¯åå‘ä¼ æ’­çš„æ¢¯åº¦æ˜¯å¦æ­£ç¡®"""
    # è®¡ç®—è§£ææ¢¯åº¦ï¼ˆé€šè¿‡åå‘ä¼ æ’­ï¼‰
    analytical_grad = layer.backward(dL_dout)
    
    # è®¡ç®—æ•°å€¼æ¢¯åº¦
    def loss_fn(W):
        layer.W = W
        return np.sum(layer.forward(x))
    
    numerical_grad = numerical_gradient(loss_fn, layer.W.copy())
    
    # æ¯”è¾ƒ
    rel_error = np.linalg.norm(analytical_grad - numerical_grad) / (
        np.linalg.norm(analytical_grad) + np.linalg.norm(numerical_grad)
    )
    
    print(f"ç›¸å¯¹è¯¯å·®: {rel_error:.2e}")
    if rel_error < 1e-5:
        print("âœ“ æ¢¯åº¦æ£€éªŒé€šè¿‡ï¼")
    else:
        print("âœ— æ¢¯åº¦æ£€éªŒå¤±è´¥ï¼éœ€è¦æ£€æŸ¥å®ç°ã€‚")
```

#### å…³é”®æ•°å­¦ç‚¹ 2ï¼šå­¦ä¹ ç‡çš„è‡ªé€‚åº”

åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå›ºå®šå­¦ä¹ ç‡å¾€å¾€ä¸å¤Ÿç†æƒ³ã€‚Adam ä¼˜åŒ–å™¨ä¸­çš„è‡ªé€‚åº”å­¦ä¹ ç‡ï¼š

$$\alpha_t = \alpha \cdot \frac{\sqrt{1-\beta_2^t}}{1-\beta_1^t} \cdot \frac{1}{\sqrt{\hat{v}_t} + \epsilon}$$

è¿™ä¸ªå…¬å¼ä½“ç°äº†æ·±å±‚çš„æ•°å­¦ï¼š
- åˆ†å­ä¸­çš„ $\sqrt{1-\beta_2^t}$ éšç€æ—¶é—´è¡°å‡ï¼Œç›¸å½“äºå­¦ä¹ ç‡é¢„çƒ­
- $\frac{1}{\sqrt{\hat{v}_t}}$ æä¾›äº†å‚æ•°ç‰¹å®šçš„è‡ªé€‚åº”ç¼©æ”¾

#### å…³é”®æ•°å­¦ç‚¹ 3ï¼šæ‰¹é‡å½’ä¸€åŒ–çš„æ•°å­¦æœ¬è´¨

æ‰¹é‡å½’ä¸€åŒ–åœ¨æ¯ä¸€å±‚çš„è¾“å…¥è¿›è¡Œæ ‡å‡†åŒ–ï¼š

$$\tilde{x}_i = \frac{x_i - \mathbb{E}[x_i]}{\sqrt{\text{Var}[x_i] + \epsilon}}$$

ç„¶åå­¦ä¹ ç¼©æ”¾å’Œå¹³ç§»å‚æ•° $\gamma$ å’Œ $\beta$ï¼š

$$y_i = \gamma \tilde{x}_i + \beta$$

**æ•°å­¦æœ¬è´¨**ï¼šè¿™æ”¹å˜äº†æŸå¤±å‡½æ•°çš„HessiançŸ©é˜µçš„æ¡ä»¶æ•°ï¼Œä½¿å…¶æ›´æ¥è¿‘ 1ï¼Œä»è€ŒåŠ å¿«æ”¶æ•›ã€‚

---

## å¸¸è§é—®é¢˜ä¸æ·±åº¦æ´å¯Ÿ

### Q1: ä¸ºä»€ä¹ˆæ¢¯åº¦ä¸‹é™èƒ½æ”¶æ•›åˆ°å¥½çš„è§£ï¼Ÿ

**ç­”æ¡ˆ**ï¼šè¿™æ¶‰åŠåˆ°**æ³›å‡½åˆ†æ**ä¸­çš„æ·±å±‚ç†è®ºã€‚å¯¹äºå¼ºå‡¸å‡½æ•°ï¼Œæ¢¯åº¦ä¸‹é™ä¿è¯çº¿æ€§æ”¶æ•›ã€‚ä½†ç¥ç»ç½‘ç»œçš„æŸå¤±å‡½æ•°é€šå¸¸**ä¸æ˜¯å‡¸çš„**ï¼Œä¸ºä»€ä¹ˆè¿˜èƒ½å·¥ä½œå‘¢ï¼Ÿ

**å…³é”®æ´å¯Ÿ**ï¼šè™½ç„¶å…¨å±€éå‡¸ï¼Œä½†æ¢¯åº¦ä¸‹é™æ‰¾åˆ°çš„å±€éƒ¨æœ€å°å€¼é€šå¸¸å·²ç»è¶³å¤Ÿå¥½ã€‚æœ€è¿‘çš„ç†è®ºç ”ç©¶è¡¨æ˜ï¼š

1. **é«˜ç»´ç©ºé—´çš„å‡ ä½•æ€§è´¨**ï¼šåœ¨é«˜ç»´ç©ºé—´ä¸­ï¼Œ"å"çš„å±€éƒ¨æœ€å°å€¼å¾ˆå°‘ã€‚å¤§å¤šæ•°ä¸´ç•Œç‚¹éƒ½æ˜¯éç‚¹ï¼Œä¸æ˜¯å±€éƒ¨æœ€å°å€¼ã€‚

2. **æŸå¤±å‡½æ•°æ™¯è§‚**ï¼ˆLoss Landscapeï¼‰çš„å¹³å¦æ€§ï¼šç¥ç»ç½‘ç»œçš„æŸå¤±å‡½æ•°è™½ç„¶éå‡¸ï¼Œä½†åœ¨æ‰¾åˆ°çš„æœ€å°å€¼é™„è¿‘é€šå¸¸å¾ˆå¹³å¦ï¼Œè¿™æ„å‘³ç€è§£çš„æ³›åŒ–èƒ½åŠ›å¥½ã€‚

3. **éšå«çš„æ­£åˆ™åŒ–**ï¼šéšæœºæ¢¯åº¦ä¸‹é™æœ¬èº«å…·æœ‰éšå«çš„æ­£åˆ™åŒ–æ•ˆæœï¼Œä½¿å¾—æ‰¾åˆ°çš„è§£æ³›åŒ–èƒ½åŠ›æ›´å¼ºã€‚

### Q2: ä¸ºä»€ä¹ˆ ReLU æ¯” Sigmoid æ›´æœ‰æ•ˆï¼Ÿ

**ç­”æ¡ˆ**ï¼šä»æ¢¯åº¦æµçš„è§’åº¦ï¼š

- **Sigmoid**ï¼š$\sigma'(x) = \sigma(x)(1-\sigma(x)) \leq 0.25$ï¼Œå¯¼æ•°å§‹ç»ˆå°äº 0.25ï¼Œåœ¨æ·±å±‚ç½‘ç»œä¸­é€ æˆæ¢¯åº¦æ¶ˆå¤±ã€‚

- **ReLU**ï¼š$\text{ReLU}'(x) = \begin{cases} 1 & x > 0 \\ 0 & x \leq 0 \end{cases}$ï¼Œå¯¼æ•°ä¸º 0 æˆ– 1ï¼Œæ²¡æœ‰è¡°å‡ã€‚

è¿™ä»ä¿¡æ¯è®ºçš„è§’åº¦ä¹Ÿå¯ä»¥ç†è§£ï¼šReLU é€šè¿‡ç¨€ç–æ¿€æ´»ï¼ˆå¤§çº¦ 50% çš„ç¥ç»å…ƒè¾“å‡º 0ï¼‰å‡å°‘äº†ä¿¡æ¯å†—ä½™ã€‚

### Q3: ä¸ºä»€ä¹ˆéœ€è¦åŠ¨é‡ï¼Ÿ

**ç­”æ¡ˆ**ï¼šä»å‡ ä½•è§’åº¦ï¼Œæ¢¯åº¦ä¸‹é™åœ¨"èµ°å»Š"å½¢çš„æŸå¤±å‡½æ•°ä¸­æŒ¯è¡å‰§çƒˆï¼š

```
     ^ æŸå¤±
     |
   --|--
    /  \        è¿™æ ·çš„å½¢çŠ¶ä¼šå¯¼è‡´
   /    \       é™¡å³­çš„å‚ç›´æ¢¯åº¦
  /______\____  å¹³ç¼“çš„æ°´å¹³æ¢¯åº¦
     å‚æ•°
```

åœ¨è¿™æ ·çš„åœ°å½¢ä¸­ï¼š
- å¦‚æœåªç”¨æ¢¯åº¦ï¼Œåœ¨å‚ç›´æ–¹å‘æŒ¯è¡å‰§çƒˆ
- åŠ å…¥åŠ¨é‡åï¼Œå‚ç›´æ–¹å‘çš„æŒ¯è¡ç›¸äº’æŠµæ¶ˆï¼Œè€Œæ°´å¹³æ–¹å‘çš„æ•ˆåº”ç´¯ç§¯

**æ•°å­¦è¡¨ç°**ï¼šåŠ¨é‡ç›¸å½“äºç»™æŸå¤±å‡½æ•°æ·»åŠ ä¸€ä¸ªè™šéƒ¨ï¼Œæ”¹å˜å…¶æ¡ä»¶æ•°ã€‚

### Q4: æ·±åº¦å­¦ä¹ ä¸­çš„"æš—ç‰©è´¨"æ˜¯ä»€ä¹ˆï¼Ÿ

**ç­”æ¡ˆ**ï¼šæŒ‡çš„æ˜¯æˆ‘ä»¬è¿˜ä¸å®Œå…¨ç†è§£çš„ç°è±¡ï¼š

1. **ç¼©æ”¾å¾‹ï¼ˆScaling Lawsï¼‰**ï¼šä¸ºä»€ä¹ˆæ¨¡å‹å¤§å°ã€æ•°æ®é‡å’Œè®¡ç®—é‡ä¹‹é—´å­˜åœ¨å¹‚å¾‹å…³ç³»ï¼Ÿ
   $$\text{Loss} \propto \left(\frac{C}{N}\right)^\alpha$$

2. **æ¶Œç°èƒ½åŠ›ï¼ˆEmergent Abilitiesï¼‰**ï¼šä¸ºä»€ä¹ˆæŸäº›èƒ½åŠ›åœ¨æ¨¡å‹è¾¾åˆ°ç‰¹å®šå¤§å°æ—¶çªç„¶å‡ºç°ï¼Ÿ

3. **åŒé‡ä¸‹é™ï¼ˆDouble Descentï¼‰**ï¼šä¸ºä»€ä¹ˆè¿‡åº¦å‚æ•°åŒ–çš„æ¨¡å‹åè€Œæ³›åŒ–æ›´å¥½ï¼Ÿ

è¿™äº›ç°è±¡æš—ç¤ºæ·±åº¦å­¦ä¹ ä¸­è¿˜æœ‰æ·±å±‚çš„æ•°å­¦ç»“æ„ç­‰å¾…å‘ç°ã€‚

---

## æ€»ç»“

æ·±åº¦å­¦ä¹ çš„ä¼˜é›…ä¹‹å¤„åœ¨äºå…¶æ•°å­¦çš„ä¸€è‡´æ€§å’Œä¼˜ç¾ï¼š

| æ¦‚å¿µ | æ•°å­¦åŸºç¡€ | å®é™…æ„ä¹‰ |
|------|---------|---------|
| åå‘ä¼ æ’­ | é“¾å¼æ³•åˆ™ | é«˜æ•ˆè®¡ç®—æ¢¯åº¦ |
| æ¢¯åº¦ä¸‹é™ | ä¸€é˜¶æ³°å‹’å±•å¼€ | å‚æ•°ä¼˜åŒ– |
| åŠ¨é‡ | ä½é€šæ»¤æ³¢ | åŠ é€Ÿæ”¶æ•› |
| Adam | é€‚åº”æ€§çŸ©ä¼°è®¡ | è‡ªé€‚åº”å­¦ä¹ ç‡ |
| äº¤å‰ç†µ | KL æ•£åº¦ | æ¦‚ç‡åŒ¹é… |
| æ­£åˆ™åŒ– | è´å¶æ–¯å…ˆéªŒ | é˜²æ­¢è¿‡æ‹Ÿåˆ |
| æ‰¹é‡å½’ä¸€åŒ– | Hessian è°ƒç† | åŠ é€Ÿè®­ç»ƒ |

è¿™äº›æ¦‚å¿µä¸æ˜¯å­¤ç«‹çš„ï¼Œè€Œæ˜¯å½¢æˆäº†ä¸€ä¸ªç»Ÿä¸€çš„æ•°å­¦æ¡†æ¶ã€‚ç†è§£è¿™äº›åŸºç¡€ï¼Œæ‰èƒ½çœŸæ­£æŒæ¡æ·±åº¦å­¦ä¹ çš„æœ¬è´¨ï¼Œè€Œä¸ä»…ä»…æ˜¯ä½¿ç”¨æ¡†æ¶ã€‚

---

## å‚è€ƒèµ„æº

- **ç»å…¸æ•™ç§‘ä¹¦**ï¼šã€ŠDeep Learningã€‹(Goodfellow et al., 2016)
- **ä¼˜åŒ–ç†è®º**ï¼šBoyd & Vandenberghe, "Convex Optimization"
- **ä¿¡æ¯è®ºåŸºç¡€**ï¼šCover & Thomas, "Elements of Information Theory"
- **ç°ä»£ç ”ç©¶**ï¼šArpit et al., "Why Does Batch Normalization Help Optimization?"

---

## é™„å½•ï¼šä»£ç è¿è¡Œè¯´æ˜

ä¸Šè¿°ä»£ç å¯ä»¥ç›´æ¥è¿è¡Œï¼Œä¼šè¾“å‡ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å€¼ä¸‹é™æƒ…å†µã€‚å…³é”®çš„å­¦ä¹ ç‚¹ï¼š

1. **é€å…ƒç´ æ“ä½œ**ï¼šç†è§£æ¯ä¸ªæ“ä½œçš„å½¢çŠ¶å˜åŒ–
2. **é“¾å¼æ³•åˆ™çš„é€’å½’**ï¼šæ¢¯åº¦å¦‚ä½•ä»è¾“å‡ºé€å±‚åå‘ä¼ æ’­
3. **å‚æ•°æ›´æ–°**ï¼šæ¢¯åº¦å¦‚ä½•è½¬åŒ–ä¸ºå‚æ•°æ”¹è¿›
4. **æ•°å€¼ç¨³å®šæ€§**ï¼šSoftmax ä¸­å‡å»æœ€å¤§å€¼é˜²æ­¢æº¢å‡º

å»ºè®®è¯»è€…åœ¨ç†è§£æ¯ä¸€éƒ¨åˆ†åï¼Œä¿®æ”¹ä»£ç å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€ç½‘ç»œç»“æ„ã€æ‰¹é‡å¤§å°ï¼‰è§‚å¯Ÿå…¶å¯¹è®­ç»ƒè¿‡ç¨‹çš„å½±å“ï¼Œè¿™æ ·ä¼šåŠ æ·±å¯¹æ•°å­¦ç†è®ºçš„ç†è§£ã€‚
