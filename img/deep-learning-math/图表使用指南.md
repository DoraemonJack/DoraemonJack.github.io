---
layout: page
title: "深度学习数学基础 - 图表使用指南"
description: "Guide for using deep learning mathematical visualizations"
header-img: "img/post-bg-2015.jpg"
hide-in-nav: true
---

## 📊 已生成的图表总览

### 核心算法与架构
| 图表 | 文件名 | 大小 | 用途 |
|------|--------|------|------|
| 感知机 vs XOR | 01_perceptron_vs_xor.png | 327KB | 理解线性不可分问题 |
| 激活函数对比 | 02_activation_functions.png | 534KB | 对比 Sigmoid/Tanh/ReLU |
| 梯度消失问题 | 03_gradient_vanishing.png | 207KB | 展示深度网络梯度衰减 |
| 损失函数对比 | 04_loss_functions.png | 407KB | MSE vs CrossEntropy |
| 学习率效果 | 05_learning_rate_effect.png | 570KB | 显示学习率的影响 |
| SGD 收敛性 | 06_sgd_convergence.png | 493KB | Full Batch vs SGD |
| MLP 架构 | 07_mlp_architecture.png | 348KB | 网络结构与参数计数 |

**总计**：约 3MB 高质量图表

---

## 🎓 学习路径

### 初级：理解基础概念
1. 先看 **01_perceptron_vs_xor.png**
   - 理解：为什么需要多层网络？
   
2. 再看 **07_mlp_architecture.png**
   - 理解：多层网络是什么样的？

### 中级：理解核心困难
3. 看 **02_activation_functions.png**
   - 理解：激活函数的重要性

4. 看 **03_gradient_vanishing.png**
   - 理解：深度网络为什么难以训练？

### 高级：优化与实践
5. 看 **04_loss_functions.png**
   - 理解：选择合适的损失函数

6. 看 **05_learning_rate_effect.png**
   - 理解：参数调优的重要性

7. 看 **06_sgd_convergence.png**
   - 理解：优化算法的对比

---

## 💡 关键洞察速查

### 激活函数
![激活函数对比详解](/img/deep-learning-math/02_activation_functions.png)

**快速结论**：
- Sigmoid: 导数 ≤ 0.25（梯度消失）
- ReLU: 导数为 0 或 1（梯度稳定）✓ 推荐用于隐藏层

### 梯度消失
![梯度消失问题演示](/img/deep-learning-math/03_gradient_vanishing.png)

**快速结论**：
- 0.25^50 ≈ 10^-30（梯度完全消失）
- 使用 ReLU 可以完全解决

### 损失函数
![损失函数对比详解](/img/deep-learning-math/04_loss_functions.png)

**快速结论**：
- 分类任务用 CrossEntropy（梯度清晰）
- 回归任务用 MSE（简单直观）

### 学习率
![学习率效果分析](/img/deep-learning-math/05_learning_rate_effect.png)

**快速结论**：
- 太小：收敛慢
- 太大：振荡/发散
- 最优：需要实验调整

### 优化算法
![SGD 收敛性对比](/img/deep-learning-math/06_sgd_convergence.png)

**快速结论**：
- Mini-Batch SGD 是标准做法
- 批量大小通常 32-256

---

## 🔧 实际应用

### 调试神经网络时
1. **网络不收敛？**
   - 检查激活函数（看图 02, 03）
   - 调整学习率（看图 05）
   - 尝试不同优化器（看图 06）

2. **梯度爆炸/消失？**
   - 对标图 03
   - 使用 ReLU + 权重初始化
   - 尝试批量归一化

3. **损失不下降？**
   - 检查损失函数选择（看图 04）
   - 验证数据预处理
   - 调整学习率（看图 05）

---

## 📈 数值参考

### 梯度衰减系数
- Sigmoid with 10 layers: 0.25^10 ≈ 10^-6
- Sigmoid with 20 layers: 0.25^20 ≈ 10^-12
- Sigmoid with 50 layers: 0.25^50 ≈ 10^-30 ❌

### 损失函数值参考
- 接近正确 (y=0.8): MSE=0.013, CE=0.223
- 完全错误 (y=0.1): MSE=0.42, CE=2.303

### 学习率参考
- 太小: 0.0001 - 0.001
- 最优: 0.001 - 0.1
- 太大: > 0.1

---

## 🎨 演讲/文档使用

所有图表均为高分辨率 (300 DPI)，适合：
- 📊 PowerPoint/Keynote 演讲
- 📄 论文/文档
- 🌐 网页博客

### 推荐排列
1. **序列化讲座**（按学习路径）：01 → 07 → 02 → 03 → 04 → 05 → 06
2. **主题化讲座**：
   - "为什么需要深度学习" → 01
   - "激活函数与梯度流" → 02, 03
   - "损失函数选择" → 04
   - "训练技巧" → 05, 06
   - "网络架构" → 07

---

## 📝 数学对应

| 图表 | 核心数学概念 |
|------|-----------|
| 01 | 线性可分性、异或问题 |
| 02 | 导数、非线性、激活函数 |
| 03 | 链式法则、指数衰减 |
| 04 | KL 散度、交叉熵、梯度 |
| 05 | 泰勒展开、步长选择 |
| 06 | 随机优化、大数定律 |
| 07 | 前向传播、参数计数 |

---

**💻 生成脚本**：`generate_visualizations_en.py`
**📅 生成日期**：2026-01-24
**✅ 所有图表均已验证**

